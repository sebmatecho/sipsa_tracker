{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIPSA PROJECT\n",
    "### Developed by [SÃ©bastien Lozano-Forero](https://www.linkedin.com/in/sebastienlozanoforero/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T19:25:33.484143Z",
     "start_time": "2024-09-28T19:25:32.278415Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.logging_setup import setup_logger\n",
    "from src.DataCollector import DataCollector\n",
    "from src.FileNameBuilder import \n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from botocore.exceptions import ClientError\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from typing import List\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## v1.0 full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Until early september, this version was the most complete dealing with data transformation for both formats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### DataCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:15:29.447368Z",
     "start_time": "2024-09-16T14:15:29.383168Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger('DataCollectorLogger')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a file handler for logging\n",
    "file_handler = logging.FileHandler('data_collector.log', mode='w')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create a logging format\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the file handler to the logger\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "class DataCollector:\n",
    "    def __init__(self, s3: boto3.resource) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the DataCollector with S3 resource and configuration parameters.\n",
    "        \"\"\"\n",
    "        self.url_base = 'https://www.dane.gov.co'\n",
    "        self.url = 'https://www.dane.gov.co/index.php/estadisticas-por-tema/agropecuario/sistema-de-informacion-de-precios-sipsa/mayoristas-boletin-semanal-1'\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"\n",
    "        }\n",
    "        self.s3 = s3\n",
    "        self.files_tracker_name = 'files_tracker.csv'\n",
    "        self.logfile_name = 'logfile'\n",
    "\n",
    "    def all_years_links(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get the set of year links available on the DANE webpage.\n",
    "        \"\"\"\n",
    "        response = requests.get(self.url, headers=self.headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        link_years = soup.find_all(lambda tag: tag.name == 'a' and re.match(r'^\\d+$', tag.get_text().strip()))\n",
    "        return link_years\n",
    "\n",
    "    def links_per_year(self, link: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get all report links for a specific year.\n",
    "        \"\"\"\n",
    "        r = requests.get(self.url_base + link['href'], headers=self.headers)\n",
    "        logger.info(f\"Working on {link['href'][-4:]} files\")\n",
    "        soup_year = BeautifulSoup(r.content, \"html.parser\")\n",
    "        one_year_links = [item for item in soup_year.find_all(target='_blank') if 'Anexo' in item.text]\n",
    "        return one_year_links\n",
    "\n",
    "    def check_file_exists_in_s3(self, bucket_name: str, file_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a file already exists in the S3 bucket.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.s3.Object(bucket_name, file_name).load()\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == '404':\n",
    "                return False\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def load_files_tracker(self, bucket_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load the files_tracker.csv from S3 or create a new DataFrame if it does not exist.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if files_tracker.csv exists in the bucket\n",
    "            obj = self.s3.Object(bucket_name, self.files_tracker_name)\n",
    "            response = obj.get()\n",
    "            tracker_df = pd.read_csv(BytesIO(response['Body'].read()))\n",
    "            logger.info(\"Loaded existing files tracker from S3.\")\n",
    "        except self.s3.meta.client.exceptions.NoSuchKey:\n",
    "            # If files_tracker.csv does not exist, initialize an empty DataFrame\n",
    "            tracker_df = pd.DataFrame(columns=['file', 'link', 'date_added'])\n",
    "            logger.info(\"No existing files tracker found in S3. Creating a new one.\")\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == '404':\n",
    "                tracker_df = pd.DataFrame(columns=['file', 'link', 'date_added'])\n",
    "                logger.info(\"No existing files tracker found in S3. Creating a new one.\")\n",
    "            else:\n",
    "                raise\n",
    "        return tracker_df\n",
    "\n",
    "    def update_files_tracker(self, df: pd.DataFrame, bucket_name: str):\n",
    "        \"\"\"\n",
    "        Update the files_tracker.csv in S3.\n",
    "        \"\"\"\n",
    "        buffer = BytesIO()\n",
    "        df.to_csv(buffer, index=False)\n",
    "        buffer.seek(0)\n",
    "        self.s3.Bucket(bucket_name).put_object(Body=buffer, Key=self.files_tracker_name)\n",
    "        logger.info(\"Files tracker updated successfully in S3.\")\n",
    "\n",
    "    def upload_or_update_dataframe_to_s3(self, df: pd.DataFrame, bucket_name: str, file_name: str):\n",
    "        \"\"\"\n",
    "        Upload or update a DataFrame as a CSV file to an S3 bucket.\n",
    "        \"\"\"\n",
    "        # Convert DataFrame to CSV in memory\n",
    "        buffer = BytesIO()\n",
    "        df.to_csv(buffer, index=False)\n",
    "        buffer.seek(0)\n",
    "        self.s3.Bucket(bucket_name).upload_fileobj(buffer, file_name, ExtraArgs={'ContentType': 'text/csv'})\n",
    "        logger.info(f\"DataFrame {file_name} uploaded successfully to S3 bucket {bucket_name}.\")\n",
    "\n",
    "    def download_files_per_year(self, link, bucket_name: str = None):\n",
    "        \"\"\"\n",
    "        Download all files for a specific year and optionally upload them directly to an S3 bucket.\n",
    "\n",
    "        Args:\n",
    "            link (str): The link for a specific year.\n",
    "            bucket_name (str, optional): The name of the S3 bucket to upload the files to. Defaults to None.\n",
    "        \"\"\"\n",
    "        links_per_year = self.links_per_year(link)\n",
    "        n = len(links_per_year)\n",
    "\n",
    "        tracker_df = self.load_files_tracker(bucket_name) if bucket_name else None\n",
    "\n",
    "        new_files_count = 0\n",
    "\n",
    "        with tqdm(total=n, desc=f\"Processing {link['href'][-4:]} files\", unit='file') as pbar:\n",
    "            for i, file in enumerate(links_per_year):\n",
    "                file_name = f'week_{n - i}_{file[\"href\"].split(\"/\")[-1]}'\n",
    "                file_link = self.url_base + file['href']\n",
    "\n",
    "                if bucket_name and file_name in tracker_df['file'].values:\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    with requests.get(file_link, headers=self.headers, stream=True) as result:\n",
    "                        result.raise_for_status()\n",
    "\n",
    "                        if bucket_name:\n",
    "                            destination_key = f'reports/{link.text.strip()}/{file_name}'\n",
    "                            self.s3.Bucket(bucket_name).upload_fileobj(result.raw, destination_key)\n",
    "\n",
    "                            new_entry = pd.DataFrame({\n",
    "                                'file': [file_name],\n",
    "                                'link': [file_link],\n",
    "                                'date_added': [datetime.datetime.today().strftime('%Y-%m-%d')]\n",
    "                            })\n",
    "                            tracker_df = pd.concat([tracker_df, new_entry], ignore_index=True)\n",
    "                            new_files_count += 1\n",
    "\n",
    "                except requests.RequestException as e:\n",
    "                    logger.error(f\"Failed to download file from {file['href']}: {e}\")\n",
    "                    continue\n",
    "                except ClientError as e:\n",
    "                    logger.error(f\"Failed to upload file {file_name} to S3 bucket {bucket_name}: {e}\")\n",
    "                    continue\n",
    "                finally:\n",
    "                    pbar.update(1)\n",
    "\n",
    "        if bucket_name:\n",
    "            self.update_files_tracker(tracker_df, bucket_name)\n",
    "            logger.info(f\"Year {link['href'][-4:]} processed: {new_files_count} new files uploaded to S3 bucket {bucket_name}.\")\n",
    "\n",
    "    def get_files(self, bucket_name: str = None):\n",
    "        \"\"\"\n",
    "        Download all files from all years and optionally upload them to an S3 bucket.\n",
    "        \"\"\"\n",
    "        all_years_links = self.all_years_links()\n",
    "        for link in all_years_links:\n",
    "            self.download_files_per_year(link, bucket_name)\n",
    "\n",
    "        # Upload log file to S3 after processing\n",
    "        if bucket_name:\n",
    "            try:\n",
    "                self.s3.Bucket(bucket_name).upload_file('data_collector.log', self.logfile_name)\n",
    "                logger.info(f\"Log file {self.logfile_name} uploaded successfully to S3 bucket {bucket_name}.\")\n",
    "            except ClientError as e:\n",
    "                logger.error(f\"Failed to upload log file to S3 bucket {bucket_name}: {e}\")\n",
    "\n",
    "    def display_files_tracker(self, bucket_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Display the DataFrame contained in the files_tracker.csv file from the S3 bucket.\n",
    "\n",
    "        Args:\n",
    "            bucket_name (str): The name of the S3 bucket where files_tracker.csv is stored.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The contents of files_tracker.csv as a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if files_tracker.csv exists in the bucket\n",
    "            obj = self.s3.Object(bucket_name, self.files_tracker_name)\n",
    "            response = obj.get()\n",
    "            tracker_df = pd.read_csv(BytesIO(response['Body'].read()))\n",
    "            logger.info(\"Loaded files_tracker.csv from S3 successfully.\")\n",
    "        except self.s3.meta.client.exceptions.NoSuchKey:\n",
    "            logger.warning(\"files_tracker.csv does not exist in S3.\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame if the file does not exist\n",
    "        except ClientError as e:\n",
    "            logger.error(f\"Failed to load files_tracker.csv from S3: {e}\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame if there was an error\n",
    "\n",
    "        # Display the DataFrame\n",
    "        return tracker_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### FileNameBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:15:30.358178Z",
     "start_time": "2024-09-16T14:15:30.336178Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "\n",
    "class FileNameBuilder(DataCollector): \n",
    "    def __init__(self, s3: boto3.resource): \n",
    "        \"\"\"\n",
    "        Initialize FileNameBuilder with S3 resource.\n",
    "\n",
    "        Args:\n",
    "            s3 (boto3.resource): The boto3 S3 resource to interact with S3.\n",
    "        \"\"\"\n",
    "        super().__init__(s3)\n",
    "\n",
    "    def first_format_paths(self, bucket_name: str) -> list:\n",
    "        \"\"\"\n",
    "        Get the paths of files in the S3 bucket that match the first format criteria.\n",
    "\n",
    "        Args:\n",
    "            bucket_name (str): The name of the S3 bucket.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of file paths that match the first format.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Fetching first format paths from bucket: {bucket_name}\")\n",
    "        bucket = self.s3.Bucket(bucket_name)\n",
    "        object_names = [obj.key for obj in bucket.objects.all()]\n",
    "        \n",
    "        first_format_years = set(['2012', '2013', '2014', '2015', '2016', '2017', '2018'])\n",
    "        final_files_paths_first = []\n",
    "\n",
    "        for path in object_names:\n",
    "            try:\n",
    "                year = str(Path(path).parts[1])\n",
    "                week = int(Path(path).stem.split('_')[1])\n",
    "\n",
    "                # Check for files in years prior to 2018 and specific weeks in 2018\n",
    "                if year in first_format_years:\n",
    "                    if year == '2018' and week > 19:\n",
    "                        continue\n",
    "                    final_files_paths_first.append(path)\n",
    "                    logger.debug(f\"File added to first format: {path}\")\n",
    "\n",
    "            except (IndexError, ValueError) as e:\n",
    "                logger.warning(f\"Error processing path {path}: {e}\")\n",
    "        \n",
    "        logger.info(f\"Found {len(final_files_paths_first)} files for the first format.\")\n",
    "        return final_files_paths_first\n",
    "\n",
    "    def second_format_paths(self, bucket_name: str) -> list:\n",
    "        \"\"\"\n",
    "        Get the paths of files in the S3 bucket that match the second format criteria.\n",
    "\n",
    "        Args:\n",
    "            bucket_name (str): The name of the S3 bucket.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of file paths that match the second format.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Fetching second format paths from bucket: {bucket_name}\")\n",
    "        bucket = self.s3.Bucket(bucket_name)\n",
    "        object_names = [obj.key for obj in bucket.objects.all()]\n",
    "        \n",
    "        second_format_years = set(['2018', '2019', '2020', '2021', '2022', '2023', '2024'])\n",
    "        final_files_paths_second = []\n",
    "\n",
    "        for path in object_names:\n",
    "            try:\n",
    "                year = str(Path(path).parts[1])\n",
    "                week = int(Path(path).stem.split('_')[1])\n",
    "\n",
    "                # Check for files in years after 2018 and specific weeks in 2018\n",
    "                if year in second_format_years:\n",
    "                    if year == '2018' and week <= 19:\n",
    "                        continue\n",
    "                    final_files_paths_second.append(path)\n",
    "                    logger.debug(f\"File added to second format: {path}\")\n",
    "\n",
    "            except (IndexError, ValueError) as e:\n",
    "                logger.warning(f\"Error processing path {path}: {e}\")\n",
    "        \n",
    "        logger.info(f\"Found {len(final_files_paths_second)} files for the second format.\")\n",
    "        return final_files_paths_second\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### DataWrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T10:25:13.219474Z",
     "start_time": "2024-09-16T10:25:13.083897Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DataWrangler(FileNameBuilder):\n",
    "    def __init__(self, \n",
    "                 bucket_name:str, \n",
    "                 s3: boto3.resource):\n",
    "        super().__init__(s3)\n",
    "        self.bucket_name = bucket_name\n",
    "        self.s3 = s3\n",
    "        self.categories_dict = categories_dict\n",
    "        self.city_to_region = city_to_region\n",
    "\n",
    "\n",
    "    def first_format_data_extraction(self,\n",
    "                                    file_path:str)-> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extracts and processes data from an Excel file stored in an S3 bucket, handling multiple file formats.\n",
    "\n",
    "        This function attempts to retrieve an Excel file from a specified S3 bucket and process its data.\n",
    "        It uses two different engines (`openpyxl` and `xlrd`) to read the file, as some files may require \n",
    "        different engines depending on their format. If both attempts fail, an error message is logged. \n",
    "        It also filters the data to only include rows where the first column contains alphabetic characters \n",
    "        (words) and is not empty.\n",
    "\n",
    "        Args:\n",
    "            s3 (boto3.resource): A Boto3 resource object used to access the S3 bucket.\n",
    "            bucket_name (str): The name of the S3 bucket where the file is stored.\n",
    "            file_path (str): The key (path) of the file within the S3 bucket.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A pandas DataFrame containing the data from the file. If an error occurs or the \n",
    "                          file cannot be read, an empty DataFrame is returned.\n",
    "\n",
    "        Processing:\n",
    "        - Retrieves the file from the specified S3 bucket.\n",
    "        - Attempts to read the file using the `openpyxl` engine for `.xlsx` files first. If that fails, \n",
    "          it tries using the `xlrd` engine for `.xls` files.\n",
    "        - Filters the DataFrame to include only rows where the first column contains alphabetic characters \n",
    "          and is not NaN.\n",
    "\n",
    "        Example Usage:\n",
    "            first_format_data_extraction(s3, bucket_name, file_path)\n",
    "\n",
    "        Notes:\n",
    "            - This function is designed to handle files stored in S3 and may require appropriate AWS \n",
    "              credentials to access the S3 bucket.\n",
    "            - It assumes that the data file is an Excel format (`.xlsx` or `.xls`), and will print a warning \n",
    "              if the file cannot be processed.\n",
    "            - The function does not currently handle other file formats (e.g., `.csv`, `.json`).\n",
    "        \"\"\"\n",
    "        bucket = self.s3.Bucket(self.bucket_name)\n",
    "        obj = bucket.Object(file_path)\n",
    "\n",
    "        # Determine file format by extension\n",
    "    #     file_extension = Path(file_path).suffix.lower()\n",
    "    #     print(f\"Processing file: {file_path}, with extension: {file_extension}\")\n",
    "\n",
    "        # Try to read the file from S3\n",
    "\n",
    "        xls_data = obj.get()['Body'].read()\n",
    "\n",
    "        try: \n",
    "            dataframe = pd.read_excel(BytesIO(xls_data), engine='openpyxl') \n",
    "        except: \n",
    "            try:\n",
    "                dataframe = pd.read_excel(BytesIO(xls_data), engine='xlrd')\n",
    "            except: \n",
    "                print(f'[INFO] {file_path} failed to download')\n",
    "\n",
    "\n",
    "        if dataframe.shape[0]>0: \n",
    "            dataframe = dataframe[dataframe[dataframe.columns[0]].apply(lambda x: bool(re.search(r'[a-zA-Z]', str(x))) and pd.notna(x))]\n",
    "\n",
    "    # Using a classifier for the extension proved to be a wrong idea as some of the files with one extension would \n",
    "    # work with command from another one. \n",
    "    #     # Handle .xlsx files with openpyxl engine\n",
    "    #     if file_extension == '.xlsx':\n",
    "    #         dataframe = pd.read_excel(BytesIO(xls_data), engine='openpyxl')\n",
    "\n",
    "    #     # Handle .xls files with xlrd engine\n",
    "    #     elif file_extension == '.xls':\n",
    "    #         dataframe = pd.read_excel(BytesIO(xls_data), engine='xlrd')\n",
    "    #     else:\n",
    "    #         print(f\"Unsupported file extension: {file_path}\")\n",
    "    #         dataframe = pd.DataFrame()  # Handle unsupported file extensions\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    def first_format_data_transformation(self,\n",
    "                                         dataframe: pd.DataFrame, \n",
    "                                         file_path:str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transforms the raw data extracted from a file into a structured format with relevant categories and products.\n",
    "\n",
    "        This function processes the raw data extracted from an Excel file, organizing it into various food categories and\n",
    "        associated products. The transformation includes:\n",
    "        - Keeping the first five columns and renaming them.\n",
    "        - Removing rows where the 'ciudad' column is null.\n",
    "        - Identifying food categories based on sections of the file marked by the word 'cuadro'.\n",
    "        - Within each food category, identifying product blocks based on blank values in the 'precio_minimo' column.\n",
    "        - Adding relevant metadata such as the week number and year from the file path.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): The raw data extracted from an Excel file.\n",
    "            file_path (str): The file path or S3 key from which the file was downloaded. Used to extract metadata such as the \n",
    "                             year and week number.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A structured DataFrame with cleaned and organized data. The resulting DataFrame will have the \n",
    "                          following columns: \n",
    "                          - 'ciudad': The city where the prices were recorded.\n",
    "                          - 'precio_minimo': The minimum price for the product.\n",
    "                          - 'precio_maximo': The maximum price for the product.\n",
    "                          - 'precio_medio': The average price for the product.\n",
    "                          - 'tendencia': The trend for the product's price.\n",
    "                          - 'categoria': The food category to which the product belongs (e.g., 'verduras_hortalizas').\n",
    "                          - 'producto': The specific product being tracked.\n",
    "                          - 'mercado': The marketplace where the product was sold (if available).\n",
    "                          - 'semana_no': The week number, extracted from the file name.\n",
    "                          - 'anho': The year, extracted from the file name.\n",
    "\n",
    "        Processing Steps:\n",
    "        - The function first cleans the data by retaining only relevant columns and removing rows with missing cities.\n",
    "        - It identifies sections of the data marked by the word 'cuadro' and treats them as different food categories.\n",
    "        - Each category is further broken down into products, where blank values in the 'precio_minimo' column indicate \n",
    "          the beginning of a new product block.\n",
    "        - City and marketplace names are cleaned and standardized.\n",
    "        - Week number and year information is added based on the file name.\n",
    "\n",
    "        Example Usage:\n",
    "            dataframe = pd.read_excel('my_file.xlsx')\n",
    "            transformed_df = first_format_data_transformation(dataframe, 'data/week_12_2023.xlsx')\n",
    "\n",
    "        Notes:\n",
    "            - The function expects that the raw data follows a specific structure with 'cuadro' as a marker for categories and \n",
    "              blank 'precio_minimo' values as markers for products within categories.\n",
    "            - It assumes that the file name contains metadata, specifically the week number and year, in the format 'week_<number>_<year>.xlsx'.\n",
    "            - The function handles exceptions silently (e.g., missing 'cuadro' titles) and skips problematic parts of the data.\n",
    "            - 'reports/2015/week_16_Anexo_13_17abr_2015.xls' fails \n",
    "        \"\"\"\n",
    "        # Keep only the first five columns and rename them\n",
    "        dataframe = dataframe.iloc[:, 0:5]\n",
    "        dataframe.columns = ['ciudad', 'precio_minimo', 'precio_maximo', 'precio_medio', 'tendencia']\n",
    "        dataframe['ciudad'] = dataframe['ciudad'].str.lower().str.replace('bogotÃ¡, d.c.', 'bogota')\n",
    "\n",
    "        # Remove rows where 'ciudad' is null\n",
    "        dataframe = dataframe[~dataframe['ciudad'].isnull()]\n",
    "\n",
    "        # This formatting would have eight food categories within the same spreadsheet divided only by a big title.\n",
    "        # Such title would include the word 'cuadro'. So, to separate categories, we look for blocks of data contained\n",
    "        # within two consecutive appearances of such words.\n",
    "\n",
    "        # Get row indexes where the word 'cuadro' is present\n",
    "        index_cuadro = dataframe[dataframe['ciudad'].str.contains('cuadro')].index\n",
    "\n",
    "        # Creating target dataframe for all data\n",
    "        df_final = pd.DataFrame()\n",
    "\n",
    "        # Iterating over food categories.\n",
    "        for i_categoria in range(len(index_cuadro) + 1):\n",
    "            # week 16 of 2015 does not have the 'cuadro' titles. \n",
    "            try:\n",
    "                # Capturing first category\n",
    "                if i_categoria == 0:\n",
    "                    dataframe_categoria = dataframe[1:index_cuadro[i_categoria]]\n",
    "                # capturing intermediate categories\n",
    "                elif (i_categoria <= 6) and (i_categoria > 0):\n",
    "                    dataframe_categoria = dataframe[index_cuadro[i_categoria - 1] + 2:index_cuadro[i_categoria]]\n",
    "                # Capturing last category\n",
    "                else:\n",
    "                    dataframe_categoria = dataframe[index_cuadro[i_categoria - 1] + 2:]\n",
    "\n",
    "                # Within each category block, add category name\n",
    "                dataframe_categoria['categoria'] = self.categories_dict[i_categoria + 1]\n",
    "\n",
    "                # within each category block, there are several products. In the whole reporting, products are very likely to\n",
    "                # contain several rows (same food item in different locations). What identifies such product blocks is the\n",
    "                # fact that the precio_minimo column will be blank. So the product data would be contain within two\n",
    "                # consecutive occurrences of blank prices.\n",
    "                index_producto = dataframe_categoria[dataframe_categoria['precio_minimo'].isnull()].index\n",
    "\n",
    "                # creating target data frame for product category\n",
    "                df_categoria_final = pd.DataFrame()\n",
    "\n",
    "                # Iterating over products within food category\n",
    "                for i_producto in range(len(index_producto)):\n",
    "\n",
    "                    # Capturing the first product in the category\n",
    "                    if i_producto == 0:\n",
    "                        dataframe_producto = dataframe_categoria.loc[\n",
    "                                            index_producto[i_producto] - 1:index_producto[i_producto + 1] - 1].reset_index(\n",
    "                            drop=True)\n",
    "\n",
    "                    # Capturing all intermediate products\n",
    "                    elif i_producto < len(index_producto) - 1:\n",
    "                        dataframe_producto = dataframe_categoria.loc[\n",
    "                                        index_producto[i_producto]:index_producto[i_producto + 1] - 1].reset_index(\n",
    "                                drop=True)\n",
    "\n",
    "                    # Capturing last product within category\n",
    "                    else:\n",
    "                        dataframe_producto = dataframe_categoria.loc[index_producto[i_producto]:].reset_index(drop=True)\n",
    "\n",
    "                    # Adding product name column to each block of products\n",
    "                    dataframe_producto['producto'] = dataframe_producto['ciudad'][0]\n",
    "\n",
    "                    # Keeping only city name under the ciudad column\n",
    "                    dataframe_producto['ciudad'] = dataframe_producto['ciudad'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n",
    "\n",
    "                    # The name of the marketplaces is included on some of the city names. So we try to retrieve it\n",
    "                    try:\n",
    "                        dataframe_producto['mercado'] = dataframe_producto['ciudad'].str.split(',').str[1].str.strip()\n",
    "                    except:\n",
    "                        dataframe_producto['mercado'] = np.nan\n",
    "\n",
    "                    # Getting a clean version of city name\n",
    "                    try:\n",
    "                        dataframe_producto['ciudad'] = dataframe_producto['ciudad'].str.split(',').str[0].str.strip()\n",
    "                    except:\n",
    "                        None\n",
    "\n",
    "                    # Dropping first row\n",
    "                    dataframe_producto = dataframe_producto.drop(0)\n",
    "\n",
    "                    # Putting together all data for products within food category\n",
    "                    df_categoria_final = pd.concat([df_categoria_final, dataframe_producto], ignore_index=True)\n",
    "\n",
    "                # Putting together all data\n",
    "                df_final = pd.concat([df_final, df_categoria_final], ignore_index=True)\n",
    "            except: \n",
    "                None\n",
    "            # Once data per file is complete, time stamps are added: year and week number\n",
    "            df_final['semana_no'] = int(Path(file_path).name.split('_')[1])  # file_path.stem[5:7]\n",
    "            df_final['anho'] = Path(file_path).stem[-4:]\n",
    "        try: \n",
    "            df_final = df_final[['producto', 'ciudad', 'precio_minimo', 'precio_maximo', 'precio_medio',\n",
    "           'tendencia', 'categoria', 'mercado', 'semana_no', 'anho']]\n",
    "        except: \n",
    "            print(f'[INFO] {file_path} has no all columns')\n",
    "\n",
    "        return df_final\n",
    "\n",
    "\n",
    "\n",
    "    def second_format_data_extraction(self, \n",
    "                                      file_path:str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extracts and processes data from an Excel file stored in an S3 bucket using multiple sheets for the second format.\n",
    "\n",
    "        This function retrieves an Excel file from the specified S3 bucket and processes its data, which is spread across\n",
    "        multiple sheets (typically 8). It handles two possible formats (`openpyxl` for `.xlsx` and `xlrd` for `.xls`), attempting \n",
    "        both methods if necessary. The function extracts the relevant data from each sheet, processes it, and returns a combined \n",
    "        DataFrame.\n",
    "\n",
    "        Args:\n",
    "            s3 (boto3.resource): A Boto3 resource object used to access the S3 bucket.\n",
    "            bucket_name (str): The name of the S3 bucket where the file is stored.\n",
    "            file_path (str): The key (path) of the file within the S3 bucket.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A combined pandas DataFrame with data extracted from all relevant sheets in the Excel file. \n",
    "                          The resulting DataFrame will have the following columns:\n",
    "                          - 'producto': The product being tracked.\n",
    "                          - 'ciudad': The city where the prices were recorded.\n",
    "                          - 'precio_minimo': The minimum price for the product.\n",
    "                          - 'precio_maximo': The maximum price for the product.\n",
    "                          - 'precio_medio': The average price for the product.\n",
    "                          - 'tendencia': The trend for the product's price.\n",
    "                          - 'categoria': The food category of the product.\n",
    "                          - 'mercado': The marketplace where the product was sold (if available).\n",
    "                          - 'semana_no': The week number, extracted from the file name.\n",
    "                          - 'anho': The year, extracted from the file name.\n",
    "\n",
    "        Processing Steps:\n",
    "        - The function retrieves the file from S3 using the provided `file_path` and reads the Excel data using both the `openpyxl`\n",
    "          and `xlrd` engines as necessary.\n",
    "        - It extracts the names of all the sheets in the file.\n",
    "        - For each sheet, it processes the data by:\n",
    "          - Renaming columns to a standardized format.\n",
    "          - Removing rows with missing city names.\n",
    "          - Cleaning city and marketplace names.\n",
    "          - Adding metadata like the product category, week number, and year from the file name.\n",
    "        - The processed data from all sheets is concatenated into a single DataFrame and returned.\n",
    "\n",
    "        Example Usage:\n",
    "            s3 = boto3.resource('s3')\n",
    "            bucket_name = 'my-bucket'\n",
    "            file_path = 'data/week_12_2023.xlsx'\n",
    "            dataframe = second_format_data_extraction(s3, bucket_name, file_path)\n",
    "\n",
    "        Notes:\n",
    "            - The function handles both `.xlsx` and `.xls` formats. It will try to read the file with `openpyxl` first, \n",
    "              then fall back to `xlrd` if needed.\n",
    "            - The function assumes that the file has a specific structure, including multiple sheets, and that the \n",
    "              sheet names can be used to extract data relevant to food categories.\n",
    "            - Errors encountered while processing specific files or sheets are logged but not raised, allowing the function \n",
    "              to continue processing other files.\n",
    "        \"\"\"\n",
    "        bucket = self.s3.Bucket(self.bucket_name)\n",
    "        obj = bucket.Object(file_path)\n",
    "        #  Read the Excel file from S3\n",
    "        xls_data = obj.get()['Body'].read()\n",
    "\n",
    "        # Create an ExcelFile object with specified encoding\n",
    "        try: \n",
    "            xl = pd.ExcelFile(BytesIO(xls_data), engine='openpyxl') \n",
    "        except: \n",
    "            try:\n",
    "                xl = pd.ExcelFile(BytesIO(xls_data), engine='xlrd')\n",
    "            except: \n",
    "                print(f'[INFO] {file_path} failed to download')\n",
    "        # Get the list of sheet names\n",
    "        ref_dic = {i: xl.sheet_names[i] for i in range(len(xl.sheet_names))}\n",
    "\n",
    "\n",
    "        full_dataframe = pd.DataFrame()\n",
    "        for index in range(1,9):\n",
    "\n",
    "            try: \n",
    "                dataframe = pd.read_excel(BytesIO(xls_data), engine='openpyxl', sheet_name = ref_dic[index]) \n",
    "            except: \n",
    "                try:\n",
    "                    dataframe = pd.read_excel(BytesIO(xls_data), engine='xlrd', sheet_name = ref_dic[index])\n",
    "                except: \n",
    "                    print(f'[INFO] {file_path} failed to download')\n",
    "\n",
    "            if file_path == 'reports/2018/week_20_Sem_12may__18may_2018.xlsx':\n",
    "                dataframe['mercado'] = dataframe['Mercado mayorista'].str.split(',').str[1].str.strip()\n",
    "                dataframe['ciudad'] = dataframe['Mercado mayorista'].str.split(',').str[0].str.strip()\n",
    "                dataframe.columns = dataframe.columns.str.lower().str.replace(' ','_').str.replace('Ã­','i').str.replace('Ã¡','a')\n",
    "            else:\n",
    "\n",
    "                if pd.isnull(dataframe.iloc[9, 0]):\n",
    "                    dataframe = dataframe.iloc[10:, :6]\n",
    "                else:\n",
    "                    dataframe = dataframe.iloc[9:, :6]\n",
    "                dataframe.columns = ['producto', 'ciudad', 'precio_minimo', 'precio_maximo', 'precio_medio', 'tendencia']\n",
    "            dataframe = dataframe[~dataframe['ciudad'].isnull()]\n",
    "            dataframe['ciudad'] = dataframe['ciudad'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n",
    "            dataframe['ciudad'] = dataframe['ciudad'].str.lower().str.replace('bogotÃ¡, d.c.', 'bogota')\n",
    "            dataframe['ciudad'] = dataframe['ciudad'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n",
    "\n",
    "            # Adding categoria and ciudad info\n",
    "            dataframe['categoria'] = categories_dict[index]\n",
    "\n",
    "\n",
    "            # The name of the marketplaces is included on some of the city names. So we try to retrieve it\n",
    "            try:\n",
    "                dataframe['mercado'] = dataframe['ciudad'].str.split(',').str[1].str.strip()\n",
    "            except:\n",
    "                dataframe['mercado'] = np.nan\n",
    "\n",
    "            # Getting a clean version of city name\n",
    "            try:\n",
    "                dataframe['ciudad'] = dataframe['ciudad'].str.split(',').str[0].str.strip()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Once data per file is complete, time stamps are added: year and week number\n",
    "            dataframe['semana_no'] = int(Path(file_path).name.split('_')[1])  # file_path.stem[5:7]\n",
    "            dataframe['anho'] = Path(file_path).stem[-4:]\n",
    "            full_dataframe = full_dataframe.append(dataframe)\n",
    "        full_dataframe = full_dataframe.reset_index(drop = True)\n",
    "\n",
    "        try: \n",
    "            full_dataframe = full_dataframe[['producto', 'ciudad', 'precio_minimo', 'precio_maximo', 'precio_medio',\n",
    "           'tendencia', 'categoria', 'mercado', 'semana_no', 'anho']]\n",
    "        except: \n",
    "            print(f'[INFO] {file_path} has not all columns')\n",
    "        return full_dataframe\n",
    "\n",
    "    def building_complete_report(self):\n",
    "        \"\"\"\n",
    "    Constructs a complete report by extracting and transforming data from two different file formats stored in an S3 bucket.\n",
    "\n",
    "    This method processes two batches of files from an S3 bucket:\n",
    "    \n",
    "    - The first batch is associated with a specific format used for files prior to and including the 19th week of 2018.\n",
    "    - The second batch is associated with files from after the 19th week of 2018.\n",
    "    \n",
    "    For each batch:\n",
    "    \n",
    "    1. The method retrieves the file paths for both formats from S3.\n",
    "    2. For the first format, it extracts the data from each file and applies the required transformations before appending it to a final DataFrame.\n",
    "    3. For the second format, it extracts the data from each file and appends it to a final DataFrame.\n",
    "    4. The method finally concatenates both DataFrames (from the two formats) into a single complete report.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A concatenated DataFrame containing all the data from both file formats, fully extracted and transformed.\n",
    "\n",
    "    Workflow:\n",
    "    - Calls `FileNameBuilder.first_format_paths()` to get file paths for the first format.\n",
    "    - Calls `FileNameBuilder.second_format_paths()` to get file paths for the second format.\n",
    "    - For each file in the first batch:\n",
    "        - Uses `first_format_data_extraction()` to extract data from the file.\n",
    "        - Uses `first_format_data_transformation()` to transform the extracted data.\n",
    "    - For each file in the second batch:\n",
    "        - Uses `second_format_data_extraction()` to extract data from the file.\n",
    "    - Finally, concatenates the processed data from both batches into one DataFrame.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing all data from both formats processed and concatenated together.\n",
    "\n",
    "    Example:\n",
    "        report = self.building_complete_report()\n",
    "    \n",
    "    \"\"\"\n",
    "        file_names = FileNameBuilder(s3 = self.s3)\n",
    "        first_format_paths_aws = file_names.first_format_paths(bucket_name = self.bucket_name)\n",
    "        second_format_paths_aws = file_names.second_format_paths(bucket_name = self.bucket_name)\n",
    "        first_format_final = pd.DataFrame()\n",
    "        print('[INFO] First batch of files')\n",
    "        # print(path_to_file_in_aws)\n",
    "        for file_path in tqdm(first_format_paths_aws):\n",
    "            dataframe = self.first_format_data_extraction(file_path = file_path)\n",
    "\n",
    "            first_format_final = first_format_final.append(\n",
    "                self.first_format_data_transformation(dataframe = dataframe, \n",
    "                                                      file_path = file_path)\n",
    "            )\n",
    "        print('[INFO] second batch of files')\n",
    "        second_format_final = pd.DataFrame()\n",
    "        for file_path in tqdm(second_format_paths_aws):\n",
    "            second_format_final = second_format_final.append(self.second_format_data_extraction(file_path = file_path))\n",
    "\n",
    "\n",
    "        return pd.concat([first_format_final,second_format_final])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## v2.0 development "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T23:56:13.186492Z",
     "start_time": "2024-09-29T23:56:13.173495Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/logging_setup.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/logging_setup.py\n",
    "\n",
    "# import logging\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Configure logging\n",
    "# def setup_logger():\n",
    "#     \"\"\"\n",
    "#     Sets up and configures a logger for the application.\n",
    "\n",
    "#     This function creates a logger that outputs log messages both to the console (stdout) and a log file. \n",
    "#     It ensures the log directory and file are created if they do not already exist.\n",
    "\n",
    "#     The log messages include the timestamp, log level, and message. The log level is set to `INFO` for \n",
    "#     both console and file handlers, meaning that messages of level `INFO` and higher will be recorded.\n",
    "\n",
    "#     Returns:\n",
    "#         logger (logging.Logger): Configured logger object for the application.\n",
    "\n",
    "#     Logging Details:\n",
    "#         - Log Directory: A directory named 'logs' is created in the current working directory if it does not exist.\n",
    "#         - Log File: Logs are written to 'sipsa_process.log' in the 'logs' directory.\n",
    "#         - Log Format: '%(asctime)s - %(levelname)s - %(message)s'\n",
    "#         - Log Levels: Both console and file handlers are set to `INFO`.\n",
    "#         - Handlers:\n",
    "#             - Console (`stdout`) Handler: Outputs logs to the console.\n",
    "#             - File Handler: Outputs logs to 'sipsa_process.log' with overwrite mode (`mode='w'`).\n",
    "    \n",
    "#     Example Usage:\n",
    "#         >>> logger = setup_logger()\n",
    "#         >>> logger.info(\"Logger is successfully configured.\")\n",
    "\n",
    "#     Last update:\n",
    "#         Sept 28, 2024. \n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Create logfile path\n",
    "#     log_path = Path.cwd()/'logs'\n",
    "#     if not log_path.exists():\n",
    "#         print(f'{log_path} created successfully')\n",
    "#         log_path.mkdir(parents = True, \n",
    "#                    exist_ok = True)\n",
    "        \n",
    "#     logger = logging.getLogger(__name__)\n",
    "#     logger.setLevel(logging.INFO)\n",
    "\n",
    "#     # Create handlers\n",
    "#     today_date = datetime.today().strftime(format = '%m_%d_%Y')\n",
    "#     c_handler = logging.StreamHandler(sys.stdout)\n",
    "#     f_handler = logging.FileHandler(log_path/f'sipsa_process_{today_date}.log', mode='w')\n",
    "\n",
    "#     # Set levels\n",
    "#     c_handler.setLevel(logging.INFO)\n",
    "#     f_handler.setLevel(logging.INFO)\n",
    "\n",
    "#     # Create formatters and add them to handlers\n",
    "#     formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "#     c_handler.setFormatter(formatter)\n",
    "#     f_handler.setFormatter(formatter)\n",
    "\n",
    "#     # Add handlers to the logger\n",
    "#     logger.addHandler(c_handler)\n",
    "#     logger.addHandler(f_handler)\n",
    "\n",
    "#     return logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### DataCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T19:45:11.560701Z",
     "start_time": "2024-09-29T19:45:11.511717Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/DataCollector.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/DataCollector.py\n",
    "\n",
    "# import boto3\n",
    "# import requests\n",
    "# import re\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# import datetime\n",
    "# from io import BytesIO\n",
    "# from typing import List\n",
    "# from botocore.exceptions import ClientError\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "# import logging\n",
    "\n",
    "# class DataCollector:    \n",
    "#     \"\"\"\n",
    "#     A class used to collect, manage, and store data files from the DANE website into an S3 bucket.\n",
    "\n",
    "#     This class interacts with the DANE website to fetch and download data files. It checks the existence\n",
    "#     of files in an S3 bucket and updates a file tracker to ensure no duplicate downloads occur. It also provides\n",
    "#     methods to upload or update files in the S3 bucket, as well as methods to display the tracking information.\n",
    "    \n",
    "\n",
    "#     Attributes:\n",
    "#         s3 (boto3.resource): An S3 resource object to interact with AWS S3.\n",
    "#         url_base (str): The base URL of the DANE website.\n",
    "#         url (str): The URL of the DANE webpage for the data.\n",
    "#         headers (dict): HTTP headers used for web requests.\n",
    "#         files_tracker_name (str): The name of the file tracker CSV in S3.\n",
    "#         logfile_name (str): The name of the log file.\n",
    "#         logger (logging.Logger): Logger instance for logging messages.\n",
    "\n",
    "#     Methods:\n",
    "#         __init__(s3: boto3.resource, logger: logging.Logger) -> None:\n",
    "#             Initializes the DataCollector with S3 resource and logger.\n",
    "\n",
    "#         all_years_links() -> List[BeautifulSoup]:\n",
    "#             Fetches the set of year links available on the DANE webpage.\n",
    "\n",
    "#         links_per_year(link: BeautifulSoup) -> List[BeautifulSoup]:\n",
    "#             Retrieves all report links for a specific year.\n",
    "\n",
    "#         check_file_exists_in_s3(bucket_name: str, file_name: str) -> bool:\n",
    "#             Checks if a specific file already exists in the S3 bucket.\n",
    "\n",
    "#         load_files_tracker(bucket_name: str) -> pd.DataFrame:\n",
    "#             Loads the files_tracker.csv from S3 or creates a new DataFrame if it does not exist.\n",
    "\n",
    "#         update_files_tracker(df: pd.DataFrame, bucket_name: str):\n",
    "#             Updates the files_tracker.csv in S3.\n",
    "\n",
    "#         upload_or_update_dataframe_to_s3(df: pd.DataFrame, bucket_name: str, file_name: str):\n",
    "#             Uploads or updates a DataFrame as a CSV file to an S3 bucket.\n",
    "\n",
    "#         download_files_per_year(link: BeautifulSoup, bucket_name: str = None):\n",
    "#             Downloads all files for a specific year and optionally uploads them to an S3 bucket.\n",
    "\n",
    "#         get_files(bucket_name: str = None):\n",
    "#             Downloads all files from all years and optionally uploads them to an S3 bucket.\n",
    "\n",
    "#         display_files_tracker(bucket_name: str) -> pd.DataFrame:\n",
    "#             Displays the DataFrame contained in the files_tracker.csv file from the S3 bucket.\n",
    "#             \"\"\"\n",
    "    \n",
    "#     def __init__(self, s3: boto3.resource, logger: logging.Logger) -> None:\n",
    "#         \"\"\"\n",
    "#         Initializes the DataCollector class with the provided S3 resource and logger.\n",
    "\n",
    "#         Args:\n",
    "#             s3 (boto3.resource): An S3 resource object to interact with AWS S3.\n",
    "#             logger (logging.Logger): Logger instance for logging messages.\n",
    "#             \"\"\"\n",
    "#         self.url_base = 'https://www.dane.gov.co'\n",
    "#         self.url = 'https://www.dane.gov.co/index.php/estadisticas-por-tema/agropecuario/sistema-de-informacion-de-precios-sipsa/mayoristas-boletin-semanal-1'\n",
    "#         self.headers = {\n",
    "#             \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "#         }\n",
    "#         self.s3 = s3\n",
    "#         self.files_tracker_name = 'files_tracker.csv'\n",
    "#         self.logfile_name = 'logfile'\n",
    "#         self.logger = logger\n",
    "\n",
    "#     def all_years_links(self) -> List[BeautifulSoup]:\n",
    "#         \"\"\"\n",
    "#         Retrieves the set of year links available on the DANE webpage.\n",
    "\n",
    "#         Returns:\n",
    "#             List[BeautifulSoup]: A list of BeautifulSoup objects representing the links for each year.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             response = requests.get(self.url, headers=self.headers)\n",
    "#             response.raise_for_status()\n",
    "#         except requests.RequestException as e:\n",
    "#             self.logger.error(f\"Failed to fetch URL {self.url}: {e}\")\n",
    "#             return []\n",
    "\n",
    "#         soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "#         link_years = soup.find_all(lambda tag: tag.name == 'a' and re.match(r'^\\d+$', tag.get_text().strip()))\n",
    "#         return link_years\n",
    "\n",
    "#     def links_per_year(self, link: BeautifulSoup) -> List[BeautifulSoup]:\n",
    "#         \"\"\"\n",
    "#         Retrieves all report links for a specific year.\n",
    "\n",
    "#         Args:\n",
    "#             link (BeautifulSoup): The BeautifulSoup object containing the link to the year page.\n",
    "\n",
    "#         Returns:\n",
    "#             List[BeautifulSoup]: A list of BeautifulSoup objects representing the report links for the specified year.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             r = requests.get(self.url_base + link['href'], headers=self.headers)\n",
    "#             r.raise_for_status()\n",
    "#         except requests.RequestException as e:\n",
    "#             self.logger.error(f\"Failed to fetch URL {self.url_base + link['href']}: {e}\")\n",
    "#             return []\n",
    "\n",
    "#         self.logger.info(f\"Working on {link['href'][-4:]} files\")\n",
    "#         soup_year = BeautifulSoup(r.content, \"html.parser\")\n",
    "#         one_year_links = [item for item in soup_year.find_all(target='_blank') if 'Anexo' in item.text]\n",
    "#         return one_year_links\n",
    "\n",
    "#     def check_file_exists_in_s3(self, bucket_name: str, file_name: str) -> bool:\n",
    "#         \"\"\"\n",
    "#         Checks if a specific file already exists in the S3 bucket.\n",
    "\n",
    "#         Args:\n",
    "#             bucket_name (str): The name of the S3 bucket.\n",
    "#             file_name (str): The name of the file to check.\n",
    "\n",
    "#         Returns:\n",
    "#             bool: True if the file exists in the S3 bucket, False otherwise.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             self.s3.Object(bucket_name, file_name).load()\n",
    "#             return True\n",
    "#         except ClientError as e:\n",
    "#             if e.response['Error']['Code'] == '404':\n",
    "#                 return False\n",
    "#             else:\n",
    "#                 self.logger.error(f\"ClientError when checking {file_name} in bucket {bucket_name}: {e}\")\n",
    "#                 raise\n",
    "\n",
    "#     def load_files_tracker(self, bucket_name: str) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Loads the files_tracker.csv from S3 or creates a new DataFrame if it does not exist.\n",
    "\n",
    "#         Args:\n",
    "#             bucket_name (str): The name of the S3 bucket containing the files_tracker.csv.\n",
    "\n",
    "#         Returns:\n",
    "#             pd.DataFrame: A DataFrame containing the file tracking information.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             obj = self.s3.Object(bucket_name, self.files_tracker_name)\n",
    "#             response = obj.get()\n",
    "#             tracker_df = pd.read_csv(BytesIO(response['Body'].read()))\n",
    "#             self.logger.info(\"Loaded existing files tracker from S3.\")\n",
    "#         except ClientError as e:\n",
    "#             if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "#                 tracker_df = pd.DataFrame(columns=['file', 'link', 'date_added'])\n",
    "#                 self.logger.info(\"No existing files tracker found in S3. Creating a new one.\")\n",
    "#             else:\n",
    "#                 self.logger.error(f\"ClientError when accessing {self.files_tracker_name}: {e}\")\n",
    "#                 raise\n",
    "#         return tracker_df\n",
    "\n",
    "#     def update_files_tracker(self, df: pd.DataFrame, bucket_name: str):\n",
    "#         \"\"\"\n",
    "#         Updates the files_tracker.csv in S3 with new file information.\n",
    "\n",
    "#         Args:\n",
    "#             df (pd.DataFrame): The DataFrame containing file tracking information to update.\n",
    "#             bucket_name (str): The name of the S3 bucket where the files_tracker.csv is stored.\n",
    "#         \"\"\"\n",
    "#         buffer = BytesIO()\n",
    "#         df.to_csv(buffer, index=False)\n",
    "#         buffer.seek(0)\n",
    "#         try:\n",
    "#             self.s3.Bucket(bucket_name).put_object(Body=buffer, Key=self.files_tracker_name)\n",
    "# #             self.logger.info(\"Files tracker updated successfully in S3.\")\n",
    "#         except ClientError as e:\n",
    "#             self.logger.error(f\"Failed to update files tracker in S3 bucket {bucket_name}: {e}\")\n",
    "#             raise\n",
    "\n",
    "#     def upload_or_update_dataframe_to_s3(self, df: pd.DataFrame, bucket_name: str, file_name: str):\n",
    "#         \"\"\"\n",
    "#         Uploads or updates a DataFrame as a CSV file to an S3 bucket.\n",
    "\n",
    "#         Args:\n",
    "#             df (pd.DataFrame): The DataFrame to be uploaded.\n",
    "#             bucket_name (str): The name of the S3 bucket to upload to.\n",
    "#             file_name (str): The name of the file to be uploaded.\n",
    "#         \"\"\"\n",
    "#         buffer = BytesIO()\n",
    "#         df.to_csv(buffer, index=False)\n",
    "#         buffer.seek(0)\n",
    "#         try:\n",
    "#             self.s3.Bucket(bucket_name).upload_fileobj(buffer, file_name, ExtraArgs={'ContentType': 'text/csv'})\n",
    "#             self.logger.info(f\"DataFrame {file_name} uploaded successfully to S3 bucket {bucket_name}.\")\n",
    "#         except ClientError as e:\n",
    "#             self.logger.error(f\"Failed to upload DataFrame {file_name} to S3 bucket {bucket_name}: {e}\")\n",
    "#             raise\n",
    "\n",
    "#     def download_files_per_year(self, link: BeautifulSoup, bucket_name: str = None):\n",
    "#         \"\"\"\n",
    "#         Downloads all files for a specific year and optionally uploads them to an S3 bucket.\n",
    "\n",
    "#         Args:\n",
    "#             link (BeautifulSoup): The BeautifulSoup object representing the year link.\n",
    "#             bucket_name (str, optional): The name of the S3 bucket to upload the files to. Defaults to None.\n",
    "#         \"\"\"\n",
    "#         links_per_year = self.links_per_year(link)\n",
    "#         n = len(links_per_year)\n",
    "\n",
    "#         tracker_df = self.load_files_tracker(bucket_name) if bucket_name else pd.DataFrame(columns=['file', 'link', 'date_added'])\n",
    "\n",
    "#         new_files_count = 0\n",
    "\n",
    "#         with tqdm(total=n, desc=f\"Processing {link['href'][-4:]} files\", unit='file') as pbar:\n",
    "#             for i, file in enumerate(links_per_year):\n",
    "#                 file_name = f'week_{n - i}_{file[\"href\"].split(\"/\")[-1]}'\n",
    "#                 file_link = self.url_base + file['href']\n",
    "\n",
    "#                 if bucket_name and file_name in tracker_df['file'].values:\n",
    "#                     pbar.update(1)\n",
    "#                     continue\n",
    "\n",
    "#                 try:\n",
    "#                     with requests.get(file_link, headers=self.headers, stream=True) as result:\n",
    "#                         result.raise_for_status()\n",
    "\n",
    "#                         if bucket_name:\n",
    "#                             destination_key = f'reports/{link.text.strip()}/{file_name}'\n",
    "#                             self.s3.Bucket(bucket_name).upload_fileobj(result.raw, destination_key)\n",
    "\n",
    "#                             new_entry = pd.DataFrame({\n",
    "#                                 'file': [file_name],\n",
    "#                                 'link': [file_link],\n",
    "#                                 'date_added': [datetime.datetime.today().strftime('%Y-%m-%d')]\n",
    "#                             })\n",
    "#                             tracker_df = pd.concat([tracker_df, new_entry], ignore_index=True)\n",
    "#                             new_files_count += 1\n",
    "\n",
    "#                 except requests.RequestException as e:\n",
    "#                     self.logger.error(f\"Failed to download file from {file_link}: {e}\")\n",
    "#                     continue\n",
    "#                 except ClientError as e:\n",
    "#                     self.logger.error(f\"Failed to upload file {file_name} to S3 bucket {bucket_name}: {e}\")\n",
    "#                     continue\n",
    "#                 finally:\n",
    "#                     pbar.update(1)\n",
    "\n",
    "#         if bucket_name:\n",
    "#             self.update_files_tracker(tracker_df, bucket_name)\n",
    "#             self.logger.info(f\"Year {link['href'][-4:]} processed: {new_files_count} new files uploaded to S3 bucket {bucket_name}.\")\n",
    "\n",
    "#     def get_files(self, bucket_name: str = None):\n",
    "#         \"\"\"\n",
    "#         Download all files from all years and optionally upload them to an S3 bucket.\n",
    "#         \"\"\"\n",
    "#         all_years_links = self.all_years_links()\n",
    "#         for link in all_years_links:\n",
    "#             self.download_files_per_year(link, bucket_name)\n",
    "\n",
    "#         # Upload log file to S3 after processing\n",
    "#         if bucket_name:\n",
    "#             try:\n",
    "#                 self.logger.info(f\"Log file {self.logfile_name} uploaded successfully to S3 bucket {bucket_name}.\")\n",
    "#             except ClientError as e:\n",
    "#                 self.logger.error(f\"Failed to upload log file to S3 bucket {bucket_name}: {e}\")\n",
    "\n",
    "#     def display_files_tracker(self, bucket_name: str) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Display the DataFrame contained in the files_tracker.csv file from the S3 bucket.\n",
    "#         \"\"\"\n",
    "#         tracker_df = self.load_files_tracker(bucket_name)\n",
    "#         return tracker_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### FileNameBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T20:26:06.343550Z",
     "start_time": "2024-09-28T20:26:06.328552Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %%writefile src/FileNameBuilder.py\n",
    "\n",
    "# from typing import List\n",
    "# import boto3\n",
    "# import logging\n",
    "# from pathlib import Path\n",
    "\n",
    "# class FileNameBuilder:\n",
    "#     \"\"\"\n",
    "#     A class used to build file paths for data files stored in an S3 bucket based on specific format criteria.\n",
    "\n",
    "#     This class interacts with an S3 bucket to identify and categorize files according to predefined format\n",
    "#     specifications. It uses criteria such as year and file type to segregate files into 'first format' and \n",
    "#     'second format' categories. This comes from DANE changing format of the report throughout the years. \n",
    "    \n",
    "#     DANE first format refers to a single tab file with information displayed with several titles through it\n",
    "#     DANE second format refers to a file containing several tabs (per food class)\n",
    "\n",
    "#     Attributes:\n",
    "#         s3 (boto3.resource): An S3 resource object to interact with AWS S3.\n",
    "#         logger (logging.Logger): Logger instance for logging messages.\n",
    "\n",
    "#     Methods:\n",
    "#         __init__(s3: boto3.resource, logger: logging.Logger):\n",
    "#             Initializes the FileNameBuilder class with the provided S3 resource and logger.\n",
    "\n",
    "#         first_format_paths(bucket_name: str) -> List[str]:\n",
    "#             Retrieves file paths from the S3 bucket that match the first format criteria.\n",
    "\n",
    "#         second_format_paths(bucket_name: str) -> List[str]:\n",
    "#             Retrieves file paths from the S3 bucket that match the second format criteria.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, \n",
    "#                  s3: boto3.resource, \n",
    "#                  logger:logging.Logger):\n",
    "#         \"\"\"\n",
    "#         Initializes the FileNameBuilder class with the provided S3 resource and logger.\n",
    "\n",
    "#         Args:\n",
    "#             s3 (boto3.resource): An S3 resource object to interact with AWS S3.\n",
    "#             logger (logging.Logger): Logger instance for logging messages.\n",
    "#         \"\"\"\n",
    "#         self.s3 = s3\n",
    "#         self.logger = logger\n",
    "\n",
    "#     def first_format_paths(self, bucket_name: str) -> List[str]:\n",
    "#         \"\"\"\n",
    "#         Retrieves file paths from the S3 bucket that match the first format criteria.\n",
    "\n",
    "#         The first format includes files from the years 2012 to 2017, and files from the year 2018 up to \n",
    "#         and including week 19. It filters files based on their extensions (.xls, .xlsx).\n",
    "\n",
    "#         Args:\n",
    "#             bucket_name (str): The name of the S3 bucket containing the files.\n",
    "\n",
    "#         Returns:\n",
    "#             List[str]: A list of file paths matching the first format criteria.\n",
    "#         \"\"\"\n",
    "#         self.logger.info(f\"Fetching first format paths from bucket: {bucket_name}\")\n",
    "#         bucket = self.s3.Bucket(bucket_name)\n",
    "#         object_names = [obj.key for obj in bucket.objects.all() if obj.key.endswith(('.xls', '.xlsx'))]\n",
    "\n",
    "#         first_format_years = {'2012', '2013', '2014', '2015', '2016', '2017'}\n",
    "#         final_files_paths_first = []\n",
    "\n",
    "#         for path in object_names:\n",
    "#             try:\n",
    "#                 parts = Path(path).parts\n",
    "#                 if len(parts) < 2:\n",
    "#                     continue\n",
    "#                 year = parts[1]\n",
    "#                 week = int(Path(path).stem.split('_')[1])\n",
    "\n",
    "#                 # Check for files in years prior to 2018\n",
    "#                 if year in first_format_years:\n",
    "#                     final_files_paths_first.append(path)\n",
    "#                     self.logger.debug(f\"File added to first format: {path}\")\n",
    "#                 elif year == '2018' and week <= 19:\n",
    "#                     final_files_paths_first.append(path)\n",
    "#                     self.logger.debug(f\"File added to first format: {path}\")\n",
    "\n",
    "#             except (IndexError, ValueError) as e:\n",
    "#                 self.logger.warning(f\"Error processing path {path}: {e}\")\n",
    "\n",
    "#         self.logger.info(f\"Found {len(final_files_paths_first)} files for the first format.\")\n",
    "#         return final_files_paths_first\n",
    "\n",
    "#     def second_format_paths(self, bucket_name: str) -> List[str]:\n",
    "#          \"\"\"\n",
    "#         Retrieves file paths from the S3 bucket that match the second format criteria.\n",
    "\n",
    "#         The second format includes files from the years 2018 (after week 19) to 2024. It filters files based \n",
    "#         on their extensions (.xls, .xlsx).\n",
    "\n",
    "#         Args:\n",
    "#             bucket_name (str): The name of the S3 bucket containing the files.\n",
    "\n",
    "#         Returns:\n",
    "#             List[str]: A list of file paths matching the second format criteria.\n",
    "#         \"\"\"\n",
    "#         self.logger.info(f\"Fetching second format paths from bucket: {bucket_name}\")\n",
    "#         bucket = self.s3.Bucket(bucket_name)\n",
    "#         object_names = [obj.key for obj in bucket.objects.all() if obj.key.endswith(('.xls', '.xlsx'))]\n",
    "\n",
    "#         second_format_years = {'2018', '2019', '2020', '2021', '2022', '2023', '2024'}\n",
    "#         final_files_paths_second = []\n",
    "\n",
    "#         for path in object_names:\n",
    "#             try:\n",
    "#                 parts = Path(path).parts\n",
    "#                 if len(parts) < 2:\n",
    "#                     continue\n",
    "#                 year = parts[1]\n",
    "#                 week = int(Path(path).stem.split('_')[1])\n",
    "\n",
    "#                 if year in second_format_years:\n",
    "#                     if year == '2018' and week <= 19:\n",
    "#                         continue\n",
    "#                     final_files_paths_second.append(path)\n",
    "#                     self.logger.debug(f\"File added to second format: {path}\")\n",
    "\n",
    "#             except (IndexError, ValueError) as e:\n",
    "#                 self.logger.warning(f\"Error processing path {path}: {e}\")\n",
    "\n",
    "#         self.logger.info(f\"Found {len(final_files_paths_second)} files for the second format.\")\n",
    "#         return final_files_paths_second\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### DataWrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T00:00:26.271185Z",
     "start_time": "2024-09-30T00:00:26.234143Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/DataWrangler.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/DataWrangler.py\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from src.FileNameBuilder import FileNameBuilder\n",
    "# from config import CATEGORIES_DICT, CITY_TO_REGION\n",
    "# import boto3\n",
    "# import logging\n",
    "# from pathlib import Path\n",
    "# from io import BytesIO\n",
    "# from tqdm import tqdm\n",
    "# import re\n",
    "\n",
    "\n",
    "# class DataWrangler(FileNameBuilder):\n",
    "#     \"\"\"\n",
    "#     A class to handle data extraction and transformation from Excel files stored in an S3 bucket.\n",
    "\n",
    "#     This class extends the FileNameBuilder class to include data wrangling functionalities such as \n",
    "#     extracting and transforming data from different formats of Excel files, constructing complete reports,\n",
    "#     and categorizing data based on predefined schemas.\n",
    "\n",
    "#     Attributes:\n",
    "#         bucket_name (str): The name of the S3 bucket to interact with.\n",
    "#         s3 (boto3.resource): An S3 resource object to interact with AWS S3.\n",
    "#         logger (logging.Logger): Logger instance for logging messages.\n",
    "#         categories_dict (dict): A dictionary mapping category indices to category names.\n",
    "#         city_to_region (dict): A dictionary mapping city names to their respective regions.\n",
    "\n",
    "#     Methods:\n",
    "#         __init__(bucket_name: str, s3: boto3.resource, logger: logging.Logger):\n",
    "#             Initializes the DataWrangler class with S3 resource, bucket name, and logger.\n",
    "\n",
    "#         first_format_data_extraction(file_path: str) -> pd.DataFrame:\n",
    "#             Extracts and processes data from an Excel file stored in an S3 bucket using the first format.\n",
    "\n",
    "#         first_format_data_transformation(dataframe: pd.DataFrame, file_path: str) -> pd.DataFrame:\n",
    "#             Transforms the raw data extracted from a file into a structured format with relevant categories and products.\n",
    "\n",
    "#         second_format_data_extraction(file_path: str) -> pd.DataFrame:\n",
    "#             Extracts and processes data from an Excel file stored in an S3 bucket using multiple sheets for the second format.\n",
    "\n",
    "#         building_complete_report() -> pd.DataFrame:\n",
    "#             Constructs a complete report by extracting and transforming data from two different file formats stored in an S3 bucket.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, \n",
    "#                  bucket_name: str, \n",
    "#                  s3: boto3.resource, \n",
    "#                  logger:logging.Logger):\n",
    "#         \"\"\"\n",
    "#         Initializes the DataWrangler class with S3 resource, bucket name, and logger.\n",
    "\n",
    "#         Args:\n",
    "#             bucket_name (str): The name of the S3 bucket to interact with.\n",
    "#             s3 (boto3.resource): An S3 resource object to interact with AWS S3.\n",
    "#             logger (logging.Logger): Logger instance for logging messages.\n",
    "#         \"\"\"\n",
    "#         FileNameBuilder.__init__(self, s3, logger)\n",
    "#         self.bucket_name = bucket_name\n",
    "#         self.s3 = s3\n",
    "#         self.logger = logger\n",
    "#         self.categories_dict = CATEGORIES_DICT\n",
    "#         self.city_to_region = CITY_TO_REGION\n",
    "        \n",
    "#     def first_format_data_extraction(self, file_path: str) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Extracts and processes data from an Excel file stored in an S3 bucket using the first format.\n",
    "\n",
    "#         This method reads Excel files stored in an S3 bucket, handling different file formats and engines.\n",
    "#         It returns a DataFrame with the extracted data or an empty DataFrame if extraction fails.\n",
    "\n",
    "#         Args:\n",
    "#             file_path (str): The path of the file in the S3 bucket.\n",
    "\n",
    "#         Returns:\n",
    "#             pd.DataFrame: A DataFrame containing the extracted data, or an empty DataFrame if extraction fails.\n",
    "#         \"\"\"\n",
    "#         bucket = self.s3.Bucket(self.bucket_name)\n",
    "#         obj = bucket.Object(file_path)\n",
    "#         xls_data = obj.get()['Body'].read()\n",
    "\n",
    "#         dataframe = None\n",
    "#         try:\n",
    "#             dataframe = pd.read_excel(BytesIO(xls_data), engine='openpyxl')\n",
    "#         except Exception as e:\n",
    "#             self.logger.debug(f\"openpyxl failed for {file_path}: {e}\")\n",
    "#         if dataframe is None:\n",
    "#             try:\n",
    "#                 dataframe = pd.read_excel(BytesIO(xls_data), engine='xlrd')\n",
    "#             except Exception as e:\n",
    "#                 self.logger.error(f\"Failed to read Excel file {file_path} with xlrd: {e}\")\n",
    "#                 return pd.DataFrame()  # Return empty DataFrame if reading fails\n",
    "\n",
    "#         if dataframe.empty:\n",
    "#             self.logger.warning(f\"No data found in {file_path}\")\n",
    "#             return pd.DataFrame()\n",
    "\n",
    "#         dataframe = dataframe[dataframe[dataframe.columns[0]].apply(\n",
    "#             lambda x: bool(re.search(r'[a-zA-Z]', str(x))) and pd.notna(x))]\n",
    "\n",
    "#         return dataframe\n",
    "\n",
    "#     def first_format_data_transformation(self, dataframe: pd.DataFrame, file_path: str) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Transforms the raw data extracted from a file into a structured format with relevant categories and products.\n",
    "\n",
    "#         This method processes the extracted data by renaming columns, identifying product categories, and cleaning city names.\n",
    "#         It returns a structured DataFrame containing the transformed data.\n",
    "\n",
    "#         Args:\n",
    "#             dataframe (pd.DataFrame): The raw extracted data as a DataFrame.\n",
    "#             file_path (str): The path of the file in the S3 bucket.\n",
    "\n",
    "#         Returns:\n",
    "#             pd.DataFrame: A DataFrame containing the transformed data, or an empty DataFrame if transformation fails.\n",
    "#         \"\"\"\n",
    "#         # Keep only the first five columns and rename them\n",
    "#         dataframe = dataframe.iloc[:, 0:5]\n",
    "#         dataframe.columns = ['ciudad', 'precio_minimo', 'precio_maximo', 'precio_medio', 'tendencia']\n",
    "#         dataframe['ciudad'] = dataframe['ciudad'].str.lower().str.replace('bogotÃ¡, d.c.', 'bogota')\n",
    "\n",
    "#         # Remove rows where 'ciudad' is null\n",
    "#         dataframe = dataframe[~dataframe['ciudad'].isnull()]\n",
    "\n",
    "#         # Get row indexes where the word 'cuadro' is present\n",
    "#         index_cuadro = dataframe[dataframe['ciudad'].str.contains('cuadro', case=False, na=False)].index.tolist()\n",
    "\n",
    "#         if not index_cuadro:\n",
    "#             self.logger.warning(f\"No 'cuadro' titles found in {file_path}\")\n",
    "#             return pd.DataFrame()  # Return empty DataFrame if no categories found\n",
    "\n",
    "#         # Create target dataframe for all data\n",
    "#         df_final = pd.DataFrame()\n",
    "\n",
    "#         # Iterate over food categories\n",
    "#         for i_categoria in range(len(index_cuadro) + 1):\n",
    "#             try:\n",
    "#                 if i_categoria == 0:\n",
    "#                     dataframe_categoria = dataframe.iloc[1:index_cuadro[i_categoria]]\n",
    "#                 elif i_categoria <= 6:\n",
    "#                     dataframe_categoria = dataframe.iloc[index_cuadro[i_categoria - 1] + 2:index_cuadro[i_categoria]]\n",
    "#                 else:\n",
    "#                     dataframe_categoria = dataframe.iloc[index_cuadro[i_categoria - 1] + 2:]\n",
    "\n",
    "#                 # Add category name\n",
    "#                 dataframe_categoria = dataframe_categoria.copy()\n",
    "#                 dataframe_categoria['categoria'] = self.categories_dict.get(i_categoria + 1, 'unknown')\n",
    "\n",
    "#                 # Identify products within category\n",
    "#                 index_producto = dataframe_categoria[dataframe_categoria['precio_minimo'].isnull()].index.tolist()\n",
    "#                 if not index_producto:\n",
    "#                     continue\n",
    "\n",
    "#                 df_categoria_final = pd.DataFrame()\n",
    "\n",
    "#                 for i_producto in range(len(index_producto)):\n",
    "#                     if i_producto < len(index_producto) - 1:\n",
    "#                         start_idx = index_producto[i_producto]\n",
    "#                         end_idx = index_producto[i_producto + 1]\n",
    "#                     else:\n",
    "#                         start_idx = index_producto[i_producto]\n",
    "#                         end_idx = None\n",
    "\n",
    "#                     dataframe_producto = dataframe_categoria.loc[start_idx:end_idx].reset_index(drop=True)\n",
    "\n",
    "#                     # Add product name\n",
    "#                     producto_name = dataframe_producto.at[0, 'ciudad']\n",
    "#                     dataframe_producto['producto'] = producto_name\n",
    "\n",
    "#                     # Clean city names\n",
    "#                     dataframe_producto['ciudad'] = dataframe_producto['ciudad'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n",
    "\n",
    "#                     # Extract marketplace if present\n",
    "#                     dataframe_producto['mercado'] = dataframe_producto['ciudad'].str.extract(r',\\s*(.*)')[0]\n",
    "#                     dataframe_producto['ciudad'] = dataframe_producto['ciudad'].str.split(',').str[0].str.strip()\n",
    "#                     dataframe_producto = dataframe_producto[~dataframe_producto['precio_medio'].isnull()]\n",
    "                    \n",
    "#                     # Drop first row (product name)\n",
    "#                     dataframe_producto = dataframe_producto.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "#                     df_categoria_final = pd.concat([df_categoria_final, dataframe_producto], ignore_index=True)\n",
    "\n",
    "#                 df_final = pd.concat([df_final, df_categoria_final], ignore_index=True)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 self.logger.error(f\"Error processing category {i_categoria} in file {file_path}: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#         if df_final.empty:\n",
    "#             self.logger.warning(f\"No data extracted from {file_path} after transformation.\")\n",
    "#             return df_final\n",
    "\n",
    "#         # Add timestamps\n",
    "#         try:\n",
    "#             df_final['semana_no'] = int(Path(file_path).stem.split('_')[1])\n",
    "#             df_final['anho'] = Path(file_path).stem[-4:]\n",
    "#         except Exception as e:\n",
    "#             self.logger.error(f\"Error extracting week and year from {file_path}: {e}\")\n",
    "#             df_final['semana_no'] = None\n",
    "#             df_final['anho'] = None\n",
    "\n",
    "#         # Reorder columns\n",
    "#         df_final = df_final[['producto', 'ciudad', 'precio_minimo', 'precio_maximo', 'precio_medio',\n",
    "#                              'tendencia', 'categoria', 'mercado', 'semana_no', 'anho']]\n",
    "        \n",
    "#         df_final= df_final[~df_final['precio_medio'].isnull()]\n",
    "        \n",
    "#         return df_final\n",
    "\n",
    "#     def second_format_data_extraction(self, file_path: str) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Extracts and processes data from an Excel file stored in an S3 bucket using multiple sheets for the second format.\n",
    "\n",
    "#         This method reads Excel files stored in an S3 bucket, handling different file formats and sheets. \n",
    "#         It returns a structured DataFrame containing the extracted data.\n",
    "\n",
    "#         Args:\n",
    "#             file_path (str): The path of the file in the S3 bucket.\n",
    "\n",
    "#         Returns:\n",
    "#             pd.DataFrame: A DataFrame containing the extracted data, or an empty DataFrame if extraction fails.\n",
    "#         \"\"\"\n",
    "#         bucket = self.s3.Bucket(self.bucket_name)\n",
    "#         obj = bucket.Object(file_path)\n",
    "#         xls_data = obj.get()['Body'].read()\n",
    "\n",
    "#         xl = None\n",
    "#         try:\n",
    "#             xl = pd.ExcelFile(BytesIO(xls_data), engine='openpyxl')\n",
    "#         except Exception as e:\n",
    "#             self.logger.debug(f\"openpyxl failed for {file_path}: {e}\")\n",
    "#         if xl is None:\n",
    "#             try:\n",
    "#                 xl = pd.ExcelFile(BytesIO(xls_data), engine='xlrd')\n",
    "#             except Exception as e:\n",
    "#                 self.logger.error(f\"Failed to read Excel file {file_path} with xlrd: {e}\")\n",
    "#                 return pd.DataFrame()\n",
    "\n",
    "#         full_dataframe = pd.DataFrame()\n",
    "#         for index in range(1, 9):\n",
    "#             sheet_name = xl.sheet_names[index]\n",
    "#             dataframe = None\n",
    "#             try:\n",
    "#                 dataframe = pd.read_excel(BytesIO(xls_data), sheet_name=sheet_name)\n",
    "#             except Exception as e:\n",
    "#                 self.logger.error(f\"Failed to read sheet {sheet_name} in {file_path}: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#             if dataframe.empty:\n",
    "#                 self.logger.warning(f\"No data found in sheet {sheet_name} of {file_path}\")\n",
    "#                 continue\n",
    "\n",
    "#             if file_path == 'reports/2018/week_20_Sem_12may__18may_2018.xlsx':\n",
    "#                 dataframe['mercado'] = dataframe['Mercado mayorista'].str.split(',').str[1].str.strip()\n",
    "#                 dataframe['ciudad'] = dataframe['Mercado mayorista'].str.split(',').str[0].str.strip()\n",
    "#                 dataframe.columns = dataframe.columns.str.lower().str.replace(' ','_').str.replace('Ã­','i').str.replace('Ã¡','a')\n",
    "#             else:\n",
    "\n",
    "#                 if pd.isnull(dataframe.iloc[9, 0]):\n",
    "#                     dataframe = dataframe.iloc[10:, :6]\n",
    "#                 else:\n",
    "#                     dataframe = dataframe.iloc[9:, :6]\n",
    "#                 dataframe.columns = ['producto', 'ciudad', 'precio_minimo', 'precio_maximo', 'precio_medio', 'tendencia']\n",
    "#             dataframe = dataframe[~dataframe['ciudad'].isnull()]\n",
    "#             dataframe['ciudad'] = dataframe['ciudad'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n",
    "#             dataframe['ciudad'] = dataframe['ciudad'].str.lower().str.replace('bogotÃ¡, d.c.', 'bogota')\n",
    "#             dataframe['ciudad'] = dataframe['ciudad'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n",
    "\n",
    "#             # Adding categoria and ciudad info\n",
    "#             dataframe['categoria'] = self.categories_dict[index]\n",
    "\n",
    "\n",
    "#             # The name of the marketplaces is included on some of the city names. So we try to retrieve it\n",
    "#             try:\n",
    "#                 dataframe['mercado'] = dataframe['ciudad'].str.split(',').str[1].str.strip()\n",
    "#             except:\n",
    "#                 dataframe['mercado'] = np.nan\n",
    "\n",
    "#             # Getting a clean version of city name\n",
    "#             try:\n",
    "#                 dataframe['ciudad'] = dataframe['ciudad'].str.split(',').str[0].str.strip()\n",
    "#             except:\n",
    "#                 pass\n",
    "#             # Once data per file is complete, time stamps are added: year and week number\n",
    "#             dataframe['semana_no'] = int(Path(file_path).name.split('_')[1])  # file_path.stem[5:7]\n",
    "#             dataframe['anho'] = Path(file_path).stem[-4:]\n",
    "          \n",
    "\n",
    "#             # Add to full_dataframe\n",
    "#             full_dataframe = pd.concat([full_dataframe, dataframe], ignore_index=True)\n",
    "\n",
    "#         if full_dataframe.empty:\n",
    "#             self.logger.warning(f\"No data extracted from {file_path} after processing all sheets.\")\n",
    "#             return full_dataframe\n",
    "\n",
    "#         # Reorder columns\n",
    "#         full_dataframe = full_dataframe[['producto', 'ciudad', 'precio_minimo', 'precio_maximo', 'precio_medio',\n",
    "#                                          'tendencia', 'categoria', 'mercado', 'semana_no', 'anho']]\n",
    "#         return full_dataframe\n",
    "\n",
    "#     def building_complete_report(self) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Constructs a complete report by extracting and transforming data from two different file formats stored in an S3 bucket.\n",
    "\n",
    "#         This method consolidates data from the two different file formats (first and second format) stored in an S3 bucket.\n",
    "#         It combines and transforms the data into a structured report for analysis or further processing.\n",
    "\n",
    "#         Returns:\n",
    "#             pd.DataFrame: A complete report DataFrame containing data from both file formats.\n",
    "#         \"\"\"\n",
    "#         first_format_paths_aws = self.first_format_paths(bucket_name=self.bucket_name)\n",
    "#         second_format_paths_aws = self.second_format_paths(bucket_name=self.bucket_name)\n",
    "\n",
    "#         first_format_final = pd.DataFrame()\n",
    "#         self.logger.info('[INFO] First batch of files')\n",
    "\n",
    "#         for file_path in tqdm(first_format_paths_aws):\n",
    "#             dataframe = self.first_format_data_extraction(file_path)\n",
    "#             if not dataframe.empty:\n",
    "#                 transformed_df = self.first_format_data_transformation(dataframe, file_path)\n",
    "#                 first_format_final = pd.concat([first_format_final, transformed_df], ignore_index=True)\n",
    "\n",
    "#         self.logger.info('[INFO] Second batch of files')\n",
    "#         second_format_final = pd.DataFrame()\n",
    "#         for file_path in tqdm(second_format_paths_aws):\n",
    "#             dataframe = self.second_format_data_extraction(file_path)\n",
    "#             second_format_final = pd.concat([second_format_final, dataframe], ignore_index=True)\n",
    "\n",
    "#         complete_report = pd.concat([first_format_final, second_format_final], ignore_index=True)\n",
    "#         return complete_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T23:17:06.778688Z",
     "start_time": "2024-09-30T23:17:00.967935Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### DataValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T23:27:14.250852Z",
     "start_time": "2024-09-29T23:27:14.230848Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/DataValidator.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/DataValidator.py\n",
    "\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# from unidecode import unidecode\n",
    "# import logging\n",
    "\n",
    "# class DataValidator:\n",
    "#     \"\"\"\n",
    "#     The DataValidator class is responsible for validating and cleaning data from a DataFrame.\n",
    "#     It provides methods to validate city names, product names, prices, trends, and categories.\n",
    "#     It also has a function to remove accents and format text, ensuring consistency and validity of the data.\n",
    "\n",
    "#     Attributes:\n",
    "#         valid_cities (list): A list of valid Colombian city names for validation.\n",
    "#         valid_products (list): A list of valid product names for validation.\n",
    "#         valid_tendencias (list): A list of valid trends ('tendencia') for validation.\n",
    "#         valid_categorias (list): A list of valid categories for validation.\n",
    "#         logger (logging.Logger): A logger instance to log information, warnings, and errors.\n",
    "\n",
    "#     Methods:\n",
    "#         validate_city(city: str) -> bool:\n",
    "#             Checks if the provided city name is valid.\n",
    "\n",
    "#         validate_product(product: str) -> bool:\n",
    "#             Checks if the provided product name is valid.\n",
    "\n",
    "#         validate_price(price) -> bool:\n",
    "#             Checks if the provided price is a non-negative integer.\n",
    "\n",
    "#         validate_tendencia(tendencia: str) -> bool:\n",
    "#             Checks if the provided trend ('tendencia') is valid.\n",
    "\n",
    "#         validate_categoria(categoria: str) -> bool:\n",
    "#             Checks if the provided category is valid.\n",
    "\n",
    "#         remove_accents_trails_caps(text: str) -> str:\n",
    "#             Removes accents, trailing spaces, and converts text to lowercase.\n",
    "\n",
    "#         validate_dataframe(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "#             Validates the entire DataFrame, removing rows that fail validation, and returns a cleaned DataFrame.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, \n",
    "#                  logger: logging.Logger):\n",
    "#         \"\"\"\n",
    "#         Initializes the DataValidator with predefined reference data for validation and a logger instance.\n",
    "\n",
    "#         Args:\n",
    "#             logger (logging.Logger): A logger instance for logging messages.\n",
    "#         \"\"\"\n",
    "#         # Load or define reference data for validation\n",
    "#         self.valid_cities = [\n",
    "#             'bogota', 'bucaramanga', 'cali', 'cartagena', 'duitama', 'ibague',\n",
    "#        'ipiales', 'medellin', 'neiva', 'pamplona', 'pasto', 'sogamoso',\n",
    "#        'tunja', 'villavicencio', 'barranquilla', 'buenaventura',\n",
    "#        'cartago', 'cucuta', 'manizales', 'monteria', 'palmira', 'pereira',\n",
    "#        'popayan', 'rionegro', 'san_gil', 'sincelejo', 'socorro', 'tulua',\n",
    "#        'valledupar', 'chiquinquira', 'marinilla', 'cajamarca',\n",
    "#        'carmen_de_viboral', 'el_santuario', 'la_ceja', 'san_vicente',\n",
    "#        'sonson', 'armenia', 'penol', 'santa_barbara', 'yarumal',\n",
    "#        'la_virginia', 'la_union', 'la_parada', 'la_dorada', 'charala',\n",
    "#        'guepsa', 'moniquira', 'puente_nacional', 'santana', 'velez',\n",
    "#        'caparrapi', 'nocaima', 'villeta', 'honda', 'ubate',\n",
    "#        'cartagena', 'yolombo',\n",
    "#        'yopal', 'malambo', 'el_carmen_de_viboral', 'santa_marta',\n",
    "#        'florencia', 'tuquerres', 'san_andres_de_tumaco', 'arauca',\n",
    "#        'consaca', 'sandona', 'ancuya', 'tibasosa',\n",
    "#        'san_sebastian_de_mariquita', 'san_vicente_ferrer'\n",
    "#         ]\n",
    "        \n",
    "#         self.valid_products = [\n",
    "#             'acelga', 'ahuyama', 'ajo', 'ajo_importado', 'aji_dulce',\n",
    "#        'aji_topito_dulce', 'apio', 'arveja_verde_en_vaina'\n",
    "#         ]\n",
    "        \n",
    "#         self.valid_tendencias = ['+', '-', '=', '++', '--', '+++', '---']\n",
    "#         self.valid_categorias = [\n",
    "#             'verduras_hortalizas', 'frutas_frescas', 'tuberculos_raices_platanos', 'granos_cereales',\n",
    "#             'huevos_lacteos', 'carnes', 'pescados', 'productos_procesados'\n",
    "#         ]\n",
    "#         self.logger = logger\n",
    "        \n",
    "#     def validate_city(self, city: str) -> bool:\n",
    "#         \"\"\"\n",
    "#         Check if the provided city name is valid.\n",
    "\n",
    "#         Args:\n",
    "#             city (str): The city name to validate.\n",
    "\n",
    "#         Returns:\n",
    "#             bool: True if the city is valid, otherwise False.\n",
    "#         \"\"\"\n",
    "#         return city in self.valid_cities\n",
    "# #         return True\n",
    "\n",
    "    \n",
    "#     def validate_product(self, product: str) -> bool:\n",
    "#         \"\"\"\n",
    "#         Check if the provided product name is valid.\n",
    "\n",
    "#         Args:\n",
    "#             product (str): The product name to validate.\n",
    "\n",
    "#         Returns:\n",
    "#             bool: True if the product is valid, otherwise False.\n",
    "#         \"\"\"\n",
    "# #         return product in self.valid_products\n",
    "#         return True\n",
    "\n",
    "#     def validate_price(self, price) -> bool:\n",
    "#         \"\"\"\n",
    "#         Check if the provided price is a non-negative integer.\n",
    "\n",
    "#         Args:\n",
    "#             price (int or float): The price value to validate.\n",
    "\n",
    "#         Returns:\n",
    "#             bool: True if the price is a non-negative integer, otherwise False.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             return price >= 0\n",
    "#         except TypeError:\n",
    "#             return False\n",
    "\n",
    "#     def validate_tendencia(self, tendencia: str) -> bool:\n",
    "#         \"\"\"\n",
    "#         Check if the provided trend ('tendencia') is valid.\n",
    "\n",
    "#         Args:\n",
    "#             tendencia (str): The trend value to validate.\n",
    "\n",
    "#         Returns:\n",
    "#             bool: True if the trend is valid, otherwise False.\n",
    "#         \"\"\"\n",
    "#         return tendencia in self.valid_tendencias\n",
    "\n",
    "#     def validate_categoria(self, categoria: str) -> bool:\n",
    "#         \"\"\"\n",
    "#         Check if the provided category is valid.\n",
    "\n",
    "#         Args:\n",
    "#             categoria (str): The category value to validate.\n",
    "\n",
    "#         Returns:\n",
    "#             bool: True if the category is valid, otherwise False.\n",
    "#         \"\"\"\n",
    "#         return categoria in self.valid_categorias\n",
    "#     # Function to remove accents\n",
    "    \n",
    "#     def remove_accents_trails_caps(self, text):\n",
    "#         \"\"\"\n",
    "#         Removes accents, converts text to lowercase, and replaces spaces with underscores.\n",
    "\n",
    "#         Args:\n",
    "#             text (str): The text to be cleaned.\n",
    "\n",
    "#         Returns:\n",
    "#             str: Cleaned text without accents, all lowercase, and spaces replaced by underscores.\n",
    "#         \"\"\"\n",
    "#         return unidecode(text.lower().replace(' ','_').replace(',','').replace('(','').replace(')',''))\n",
    "\n",
    "#     def validate_dataframe(self, dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Validates the entire DataFrame, removing rows that fail validation.\n",
    "#         It applies all individual validation methods to ensure that the data is consistent.\n",
    "\n",
    "#         Args:\n",
    "#             dataframe (pd.DataFrame): The DataFrame to validate.\n",
    "\n",
    "#         Returns:\n",
    "#             pd.DataFrame: A DataFrame containing only valid rows. If no rows are valid, returns an empty DataFrame.\n",
    "#         \"\"\"\n",
    "#         try: \n",
    "#             # Validate each column and store the valid rows in a new DataFrame\n",
    "#             dataframe['ciudad'] = dataframe['ciudad'].apply(self.remove_accents_trails_caps)\n",
    "#             dataframe['producto'] = dataframe['producto'].apply(self.remove_accents_trails_caps)\n",
    "\n",
    "#             valid_df = dataframe[\n",
    "#                 dataframe['ciudad'].apply(self.validate_city) &\n",
    "#                 dataframe['producto'].apply(self.validate_product) &\n",
    "#                 dataframe['precio_minimo'].apply(self.validate_price) &\n",
    "#                 dataframe['precio_maximo'].apply(self.validate_price) &\n",
    "#                 dataframe['precio_medio'].apply(self.validate_price) &\n",
    "#                 dataframe['tendencia'].apply(self.validate_tendencia) &\n",
    "#                 dataframe['categoria'].apply(self.validate_categoria)\n",
    "#             ]\n",
    "\n",
    "#             # Log the rows that were removed\n",
    "#             invalid_rows = dataframe[~dataframe.index.isin(valid_df.index)]\n",
    "#             if not invalid_rows.empty:\n",
    "#                 self.logger.warning(f\"Invalid rows found and removed\")\n",
    "\n",
    "#             return valid_df\n",
    "#         except: \n",
    "#             dataframe = pd.DataFrame()\n",
    "#             return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### DataIngestor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T22:28:59.162473Z",
     "start_time": "2024-09-29T22:28:59.154508Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE product_prices (\n",
      "    producto VARCHAR(255),\n",
      "    ciudad VARCHAR(255),\n",
      "    precio_minimo INT,\n",
      "    precio_maximo INT,\n",
      "    precio_medio INT,\n",
      "    tendencia VARCHAR(10),\n",
      "    categoria VARCHAR(255),\n",
      "    mercado VARCHAR(255),\n",
      "    semana_no INT,\n",
      "    anho INT\n",
      ");\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# table_name = 'product_prices'\n",
    "text = f\"\"\"\n",
    "CREATE TABLE {table_name} (\n",
    "    producto VARCHAR(255),\n",
    "    ciudad VARCHAR(255),\n",
    "    precio_minimo INT,\n",
    "    precio_maximo INT,\n",
    "    precio_medio INT,\n",
    "    tendencia VARCHAR(10),\n",
    "    categoria VARCHAR(255),\n",
    "    mercado VARCHAR(255),\n",
    "    semana_no INT,\n",
    "    anho INT\n",
    ");\n",
    "\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T22:10:21.638654Z",
     "start_time": "2024-09-29T22:10:21.605655Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/DataIngestor.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/DataIngestor.py\n",
    "\n",
    "# import sqlalchemy\n",
    "# from sqlalchemy import create_engine\n",
    "# import logging\n",
    "# import pandas as pd\n",
    "# from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# class DataIngestor:\n",
    "#     \"\"\"\n",
    "#     The DataIngestor class is responsible for inserting data from a Pandas DataFrame into a PostgreSQL database.\n",
    "#     It provides methods to handle the insertion of large DataFrames into a specified table using the SQLAlchemy engine.\n",
    "\n",
    "#     Attributes:\n",
    "#         engine (sqlalchemy.engine.base.Engine): The SQLAlchemy engine used to connect to the PostgreSQL database.\n",
    "#         logger (logging.Logger): A logger instance to log information, warnings, and errors.\n",
    "\n",
    "#     Methods:\n",
    "#         insert_dataframe_to_db(dataframe: pd.DataFrame, table_name: str) -> None:\n",
    "#             Inserts the contents of a DataFrame into a specified PostgreSQL table in chunks.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, \n",
    "#                  engine:sqlalchemy.engine.base.Engine,\n",
    "#                 logger:logging.Logger)-> None:\n",
    "#         \"\"\"\n",
    "#         Initializes the DataIngestor with a SQLAlchemy engine and a logger instance.\n",
    "\n",
    "#         Args:\n",
    "#             engine (sqlalchemy.engine.base.Engine): The SQLAlchemy engine for the database connection.\n",
    "#             logger (logging.Logger): A logger instance for logging messages.\n",
    "#         \"\"\"\n",
    "#         self.engine = engine\n",
    "#         self.logger = logger\n",
    "        \n",
    "#     def insert_dataframe_to_db(self,\n",
    "#                                dataframe: pd.DataFrame, \n",
    "#                                table_name: str)-> None:\n",
    "    \n",
    "#         \"\"\"\n",
    "#         Inserts a DataFrame into a PostgreSQL table. Handles large DataFrames by inserting data in chunks.\n",
    "\n",
    "#         Args:\n",
    "#             dataframe (pd.DataFrame): The DataFrame to be inserted into the database.\n",
    "#             table_name (str): The name of the PostgreSQL table where the data will be inserted.\n",
    "\n",
    "#         Returns:\n",
    "#             None\n",
    "\n",
    "#         Raises:\n",
    "#             SQLAlchemyError: If an error occurs while inserting data into the database.\n",
    "\n",
    "#         Example:\n",
    "#             # Create an engine and logger\n",
    "#             engine = create_engine(\"postgresql+psycopg2://user:password@localhost:5432/mydatabase\")\n",
    "#             logger = logging.getLogger(__name__)\n",
    "            \n",
    "#             # Initialize DataIngestor\n",
    "#             ingestor = DataIngestor(engine, logger)\n",
    "            \n",
    "#             # Example DataFrame to insert\n",
    "#             dataframe = pd.DataFrame({'column1': [1, 2], 'column2': ['A', 'B']})\n",
    "            \n",
    "#             # Insert DataFrame into table 'my_table'\n",
    "#             ingestor.insert_dataframe_to_db(dataframe=dataframe, table_name='table_name')\n",
    "#         \"\"\"\n",
    "\n",
    "#         try:\n",
    "#             # Insert data in chunks to handle large DataFrames\n",
    "#             dataframe.to_sql(table_name, \n",
    "#                              self.engine, \n",
    "#                              if_exists='append', \n",
    "#                              index=False, \n",
    "#                              chunksize=500)\n",
    "\n",
    "#             self.logger.info(f\"Data successfully inserted into {table_name}.\")\n",
    "\n",
    "#         except SQLAlchemyError as e:\n",
    "#             self.logger.error(f\"Error inserting data: {e}\")\n",
    "#             raise\n",
    "\n",
    "#         finally:\n",
    "#             # Close the connection\n",
    "#             self.engine.dispose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### ProcessHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T18:25:42.219705Z",
     "start_time": "2024-09-29T18:25:42.195689Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ProcessHandler.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/ProcessHandler.py\n",
    "\n",
    "# from src.DataCollector import DataCollector\n",
    "# from src.DataWrangler import DataWrangler\n",
    "# from src.FileNameBuilder import FileNameBuilder\n",
    "# from src.DataValidator import DataValidator\n",
    "# from src.DataIngestor import DataIngestor\n",
    "\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# from pathlib import Path\n",
    "# import boto3\n",
    "# import logging\n",
    "# import sqlalchemy\n",
    "\n",
    "# class ProcessHandler(DataWrangler, DataIngestor, DataCollector, DataValidator, FileNameBuilder):\n",
    "#     \"\"\"\n",
    "#     The ProcessHandler class orchestrates the entire process of collecting, transforming, validating, \n",
    "#     and ingesting data from S3 into a PostgreSQL database. It combines functionalities from multiple classes \n",
    "#     to handle different stages of data processing, ensuring efficient and robust data management.\n",
    "\n",
    "#     Inheritance:\n",
    "#         DataWrangler: For data extraction and transformation.\n",
    "#         DataIngestor: For data insertion into PostgreSQL.\n",
    "#         DataCollector: For collecting and tracking files from S3.\n",
    "#         DataValidator: For validating data against predefined criteria.\n",
    "#         FileNameBuilder: For constructing paths for files in the S3 bucket.\n",
    "\n",
    "#     Attributes:\n",
    "#         s3 (boto3.resource): The S3 resource for interacting with AWS S3.\n",
    "#         engine (sqlalchemy.engine.base.Engine): The SQLAlchemy engine for the PostgreSQL database connection.\n",
    "#         bucket_name (str): The name of the S3 bucket to interact with.\n",
    "#         table_name (str): The name of the PostgreSQL table to which data will be ingested.\n",
    "#         logger (logging.Logger): Logger instance for logging process information.\n",
    "#         files_tracker_df (pd.DataFrame): DataFrame tracking processed files and their status.\n",
    "\n",
    "#     Methods:\n",
    "#         __init__(self, s3, engine, bucket_name, table_name, logger):\n",
    "#             Initializes the ProcessHandler with necessary attributes and loads the file tracker.\n",
    "\n",
    "#         executing_process(self, output_dataframe: bool = False) -> pd.DataFrame:\n",
    "#             Executes the data processing workflow, including data extraction, transformation, validation, and ingestion.\n",
    "\n",
    "#         update_files_tracker_with_rds_load(self, file_name: str):\n",
    "#             Updates the file tracker in S3 with the status of files loaded into the RDS.\n",
    "\n",
    "#         querying_db(self, query: str) -> pd.DataFrame:\n",
    "#             Executes a query on the PostgreSQL database and returns the result as a DataFrame.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, \n",
    "#                  s3:boto3.resource, \n",
    "#                  engine:sqlalchemy.engine.base.Engine, \n",
    "#                  bucket_name:str, \n",
    "#                  table_name:str, \n",
    "#                  logger:logging.Logger):\n",
    "#         \"\"\"\n",
    "#         Initializes the ProcessHandler with necessary resources and configurations.\n",
    "\n",
    "#         Args:\n",
    "#             s3 (boto3.resource): The S3 resource to interact with AWS S3.\n",
    "#             engine (sqlalchemy.engine.base.Engine): The SQLAlchemy engine for the PostgreSQL database connection.\n",
    "#             bucket_name (str): The name of the S3 bucket.\n",
    "#             table_name (str): The name of the table in the PostgreSQL database.\n",
    "#             logger (logging.Logger): Logger instance for logging information.\n",
    "\n",
    "#         Initializes the base classes and sets up the files tracker.\n",
    "#         \"\"\"\n",
    "#         # Initialize DataCollector and other base classes\n",
    "        \n",
    "#         DataCollector.__init__(self, s3, logger)\n",
    "#         DataIngestor.__init__(self, engine, logger)\n",
    "#         DataWrangler.__init__(self, bucket_name, s3, logger)\n",
    "#         DataValidator.__init__(self, logger)\n",
    "#         FileNameBuilder.__init__(self, s3, logger)\n",
    "#         # Set class attributes\n",
    "#         self.s3 = s3\n",
    "#         self.engine = engine\n",
    "#         self.bucket_name = bucket_name\n",
    "#         self.table_name = table_name\n",
    "#         self.logger = logger\n",
    "\n",
    "#         # Load files tracker after initializing the DataCollector\n",
    "#         self.files_tracker_df = self.load_files_tracker(self.bucket_name)  \n",
    "        \n",
    "#         # Ensure 'rds_load' column exists in the files_tracker_df\n",
    "#         if 'rds_load' not in self.files_tracker_df.columns:\n",
    "#             self.files_tracker_df['rds_load'] = 'no'\n",
    "        \n",
    "# #         self.data_validator = DataValidator()  # Initialize the DataValidator\n",
    "\n",
    "#     def executing_process(self, output_dataframe: bool = False) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Executes the complete data processing workflow, including data extraction, transformation, validation, \n",
    "#         and ingestion into the PostgreSQL database. Only processes files not marked as 'rds_load' in the files tracker.\n",
    "\n",
    "#         Args:\n",
    "#             output_dataframe (bool): If True, returns the final concatenated DataFrame from all processed files.\n",
    "\n",
    "#         Returns:\n",
    "#             pd.DataFrame: The concatenated DataFrame of all processed files if output_dataframe is True. \n",
    "#                           Otherwise, returns None.\n",
    "#         \"\"\"\n",
    "#         # Fetch all files from the source\n",
    "#         self.get_files(self.bucket_name)\n",
    "\n",
    "#         # Generate paths for the different file formats\n",
    "#         first_format_paths_aws = self.first_format_paths(bucket_name=self.bucket_name)\n",
    "#         second_format_paths_aws = self.second_format_paths(bucket_name=self.bucket_name)\n",
    "\n",
    "#         first_format_final = pd.DataFrame()\n",
    "#         self.logger.info('Started working on first batch of files')\n",
    "\n",
    "#         # Process files in the first format\n",
    "#         for file_path in tqdm(first_format_paths_aws):\n",
    "            \n",
    "#             file_name = Path(file_path).name\n",
    "#             # Skip files that are already loaded into RDS\n",
    "#             if not self.files_tracker_df.empty and self.files_tracker_df.loc[self.files_tracker_df['file'] == file_name, 'rds_load'].values[0] == 'yes':\n",
    "# #                 logger.info(f\"[INFO] Skipping file {file_name} as it is already loaded into RDS.\")\n",
    "#                 continue\n",
    "\n",
    "#             dataframe = self.first_format_data_extraction(file_path)\n",
    "#             if not dataframe.empty:\n",
    "#                 transformed_df = self.first_format_data_transformation(dataframe, file_path)\n",
    "\n",
    "#                 # Validate DataFrame before inserting into the database\n",
    "#                 valid_df = self.validate_dataframe(transformed_df)\n",
    "\n",
    "#                 if output_dataframe:\n",
    "#                     first_format_final = pd.concat([first_format_final, transformed_df], ignore_index=True)\n",
    "#                 self.insert_dataframe_to_db(dataframe=valid_df, table_name=self.table_name)\n",
    "#                 self.update_files_tracker_with_rds_load(file_name)  # Update tracker after successful load\n",
    "#         self.logger.info('Started working on second batch of files')\n",
    "#         second_format_final = pd.DataFrame()\n",
    "        \n",
    "#         # Process files in the second format\n",
    "#         for file_path in tqdm(second_format_paths_aws):\n",
    "#             file_name = Path(file_path).name\n",
    "#             # Skip files that are already loaded into RDS\n",
    "#             if not self.files_tracker_df.empty and self.files_tracker_df.loc[self.files_tracker_df['file'] == file_name, 'rds_load'].values[0] == 'yes':\n",
    "# #                 logger.info(f\"[INFO] Skipping file {file_name} as it is already loaded into RDS.\")\n",
    "#                 continue\n",
    "\n",
    "#             dataframe = self.second_format_data_extraction(file_path)\n",
    "            \n",
    "#             # Validate DataFrame before inserting into the database\n",
    "#             valid_df = self.validate_dataframe(dataframe)\n",
    "            \n",
    "#             self.insert_dataframe_to_db(dataframe=valid_df, table_name=self.table_name)\n",
    "#             self.update_files_tracker_with_rds_load(file_name)  # Update tracker after successful load\n",
    "#             if output_dataframe:\n",
    "#                 second_format_final = pd.concat([second_format_final, dataframe], ignore_index=True)\n",
    "\n",
    "#         if output_dataframe:\n",
    "#             complete_report = pd.concat([first_format_final, second_format_final], ignore_index=True)\n",
    "#             return complete_report\n",
    "\n",
    "\n",
    "#     def update_files_tracker_with_rds_load(self, file_name: str):\n",
    "#         \"\"\"\n",
    "#         Updates the 'rds_load' status in the files tracker to 'yes' after successful insertion into the RDS.\n",
    "\n",
    "#         Args:\n",
    "#             file_name (str): The name of the file to update in the files tracker.\n",
    "\n",
    "#         Returns:\n",
    "#             None\n",
    "#         \"\"\"\n",
    "#         # Check if the file is already present in the tracker and update it\n",
    "#         if file_name in self.files_tracker_df['file'].values:\n",
    "#             self.files_tracker_df.loc[self.files_tracker_df['file'] == file_name, 'rds_load'] = 'yes'\n",
    "#         else:\n",
    "#             # If not present, add a new entry\n",
    "#             new_entry = pd.DataFrame({'file': [file_name], 'rds_load': ['yes']})\n",
    "#             self.files_tracker_df = pd.concat([self.files_tracker_df, new_entry], ignore_index=True)\n",
    "        \n",
    "#         # Update the tracker in S3\n",
    "#         self.update_files_tracker(self.files_tracker_df, self.bucket_name)\n",
    "\n",
    "#     def querying_db(self, query: str) -> pd.DataFrame:\n",
    "#         \"\"\"\n",
    "#         Executes a SQL query on the PostgreSQL database and returns the result as a DataFrame.\n",
    "\n",
    "#         Args:\n",
    "#             query (str): The SQL query to execute.\n",
    "\n",
    "#         Returns:\n",
    "#             pd.DataFrame: The result of the query as a Pandas DataFrame.\n",
    "\n",
    "#         Example:\n",
    "#             query = \"SELECT * FROM product_prices WHERE ciudad = 'bogota'\"\n",
    "#             result_df = self.querying_db(query=query)\n",
    "#         \"\"\"\n",
    "#         # Running query and importing it \n",
    "#         with self.engine.begin() as conn:\n",
    "#             df = pd.read_sql(sql=query, con=conn)\n",
    "\n",
    "#         print(f'[Info] Data Frame with {df.shape[0]} rows and {df.shape[1]} columns imported successfully.')\n",
    "#         return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T19:31:56.420822Z",
     "start_time": "2024-09-28T19:31:56.412815Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile main.py\n",
    "\n",
    "# from src.logging_setup import setup_logger\n",
    "# from src.ProcessHandler import ProcessHandler\n",
    "# from dotenv import load_dotenv\n",
    "# from sqlalchemy import create_engine\n",
    "# import boto3\n",
    "# import os\n",
    "\n",
    "# # Loading credentials\n",
    "# load_dotenv()\n",
    "# aws_access_key_id = os.environ['aws_access_key_id']\n",
    "# aws_secret_access_key = os.environ['aws_secret_access_key']\n",
    "    \n",
    "# db_user = os.environ['db_user']\n",
    "# db_pass = os.environ['db_pass']\n",
    "# db_host = os.environ['db_host']\n",
    "# db_port = os.environ['db_port']\n",
    "# db_name = os.environ['db_name']\n",
    "\n",
    "# table_name = os.environ['table_name']\n",
    "# bucket_name = os.environ['bucket_name']\n",
    "\n",
    "\n",
    "# # Creating connection to database\n",
    "# engine = create_engine(f'postgresql://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}')\n",
    "\n",
    "# # Creating boto3 session (access the S3 bucket)\n",
    "# s3 = boto3.resource('s3',\n",
    "#                     aws_access_key_id = aws_access_key_id, \n",
    "#                     aws_secret_access_key = aws_secret_access_key)\n",
    "\n",
    "# # Initialize logger\n",
    "# logger = setup_logger()\n",
    "\n",
    "\n",
    "# sipsa_process = ProcessHandler(s3 = s3, \n",
    "#                                engine = engine, \n",
    "#                                bucket_name = bucket_name, \n",
    "#                                table_name = table_name, \n",
    "#                                logger = logger)\n",
    "\n",
    "# sipsa_process.executing_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T23:19:24.655603Z",
     "start_time": "2024-09-24T23:19:24.650602Z"
    }
   },
   "source": [
    "## testing playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T18:58:49.561200Z",
     "start_time": "2024-09-28T18:58:49.558201Z"
    }
   },
   "outputs": [],
   "source": [
    "# names = FileNameBuilder(s3 = s3, logger = logger)\n",
    "# first_format = names.first_format_paths(bucket_name = bucket_name)\n",
    "# second_format = names.second_format_paths(bucket_name = bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T18:58:49.656199Z",
     "start_time": "2024-09-28T18:58:49.652199Z"
    }
   },
   "outputs": [],
   "source": [
    "# j = random.randint(a = 0, b = len(second_format))\n",
    "# file_path = second_format[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T18:32:48.114689Z",
     "start_time": "2024-09-28T18:32:48.106651Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T18:58:49.773200Z",
     "start_time": "2024-09-28T18:58:49.769205Z"
    }
   },
   "outputs": [],
   "source": [
    "# testing_validation = DataWrangler(bucket_name = bucket_name, \n",
    "#                                   s3 = s3, \n",
    "#                                   logger = logger )\n",
    "# validator = DataValidator(logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T18:58:49.900200Z",
     "start_time": "2024-09-28T18:58:49.892199Z"
    }
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# i = random.randint(a = 0, b = len(first_format))\n",
    "# file_path = first_format[i]\n",
    "# # file_path = 'reports/2015/week_16_Anexo_13_17abr_2015.xls'\n",
    "# # dataframe = first_format_data_extraction(file_path = file_path)\n",
    "# dataframe = testing_validation.first_format_data_extraction(file_path = file_path)\n",
    "\n",
    "# dataframe = testing_validation.first_format_data_transformation(dataframe = dataframe, \n",
    "#                                                     file_path = file_path)\n",
    "\n",
    "# dataframe = validator.validate_dataframe(dataframe = dataframe)\n",
    "\n",
    "# dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T18:58:50.090201Z",
     "start_time": "2024-09-28T18:58:50.085200Z"
    }
   },
   "outputs": [],
   "source": [
    "# i = random.randint(a = 0, b = len(second_format))\n",
    "# file_path = second_format[i]\n",
    "# dataframe = testing_validation.second_format_data_extraction(file_path = file_path)\n",
    "# dataframe = validator.validate_dataframe(dataframe = dataframe)\n",
    "# dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T18:44:01.368153Z",
     "start_time": "2024-10-02T18:44:00.248327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-02 13:44:01,298 - INFO - Loaded existing files tracker from S3.\n",
      "2024-10-02 13:44:01,298 - INFO - Loaded existing files tracker from S3.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.logging_setup import setup_logger\n",
    "from src.ProcessHandler import ProcessHandler\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Loading credentials\n",
    "load_dotenv()\n",
    "aws_access_key_id = os.environ['aws_access_key_id']\n",
    "aws_secret_access_key = os.environ['aws_secret_access_key']\n",
    "    \n",
    "db_user = os.environ['db_user']\n",
    "db_pass = os.environ['db_pass']\n",
    "db_host = os.environ['db_host']\n",
    "db_port = os.environ['db_port']\n",
    "db_name = os.environ['db_name']\n",
    "\n",
    "table_name = os.environ['table_name']\n",
    "bucket_name = os.environ['bucket_name']\n",
    "\n",
    "\n",
    "# Creating connection to database\n",
    "engine = create_engine(f'postgresql://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}')\n",
    "\n",
    "# Creating boto3 session (access the S3 bucket)\n",
    "s3 = boto3.resource('s3',\n",
    "                    aws_access_key_id = aws_access_key_id, \n",
    "                    aws_secret_access_key = aws_secret_access_key)\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logger()\n",
    "\n",
    "\n",
    "sipsa_process = ProcessHandler(s3 = s3, \n",
    "                               engine = engine, \n",
    "                               bucket_name = bucket_name, \n",
    "                               table_name = table_name, \n",
    "                               logger = logger)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T18:44:04.502809Z",
     "start_time": "2024-10-02T18:44:04.484485Z"
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT * FROM {table_name} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T18:45:48.343345Z",
     "start_time": "2024-10-02T18:44:05.043128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Data Frame with 2627989 rows and 10 columns imported successfully.\n"
     ]
    }
   ],
   "source": [
    "    df = sipsa_process.querying_db(query = query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T18:46:18.745090Z",
     "start_time": "2024-10-02T18:46:18.341409Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['acelga', 'ahuyama', 'ajo', 'ajo_importado', 'aji_dulce',\n",
       "       'aji_topito_dulce', 'apio', 'arveja_verde_en_vaina',\n",
       "       'arveja_verde_en_vaina_pastusa', 'berenjena', 'brocoli',\n",
       "       'calabacin', 'calabaza', 'cebolla_cabezona_blanca',\n",
       "       'cebolla_cabezona_blanca_bogotana',\n",
       "       'cebolla_cabezona_blanca_importada',\n",
       "       'cebolla_cabezona_blanca_pastusa',\n",
       "       'cebolla_cabezona_blanca_peruana',\n",
       "       'cebolla_cabezona_roja_importada', 'cebolla_cabezona_roja_ocanera',\n",
       "       'cebolla_cabezona_roja_peruana', 'cebolla_junca',\n",
       "       'cebolla_junca_aquitania', 'cebolla_junca_berlin',\n",
       "       'cebolla_junca_tenerife', 'cebolla_junca_pastusa',\n",
       "       'cebolla_puerro', 'cebollin_chino', 'chocolo_mazorca', 'cidra',\n",
       "       'cilantro', 'coles', 'coliflor', 'espinaca', 'frijol_verde_bolo',\n",
       "       'frijol_verde_cargamanto', 'frijol_verde_en_vaina', 'haba_verde',\n",
       "       'habichuela', 'habichuela_larga', 'lechuga_batavia',\n",
       "       'lechuga_crespa_morada', 'lechuga_crespa_verde', 'pepino_cohombro',\n",
       "       'pepino_de_rellenar', 'perejil', 'pimenton', 'pimenton_verde',\n",
       "       'remolacha', 'remolacha_bogotana', 'remolacha_regional',\n",
       "       'repollo_blanco', 'repollo_blanco_bogotano',\n",
       "       'repollo_blanco_valluno', 'repollo_morado',\n",
       "       'repollo_verde_regional', 'rabano_rojo', 'tomate_riogrande',\n",
       "       'tomate_riogrande_bumangues', 'tomate_riogrande_ocanero',\n",
       "       'tomate_chonto', 'tomate_chonto_antioqueno',\n",
       "       'tomate_chonto_valluno', 'tomate_larga_vida', 'tomate_rinon',\n",
       "       'zanahoria', 'zanahoria_bogotana', 'zanahoria_larga_vida',\n",
       "       'aguacate_hass', 'aguacate_comun', 'aguacate_papelillo', 'badea',\n",
       "       'banano_uraba', 'banano_bocadillo', 'banano_criollo', 'borojo',\n",
       "       'breva', 'ciruela_negra_chilena', 'ciruela_roja', 'coco',\n",
       "       'curuba_larga', 'curuba_redonda', 'durazno_importado',\n",
       "       'durazno_nacional', 'feijoa', 'fresa', 'granadilla', 'guanabana',\n",
       "       'guayaba_agria', 'guayaba_comun', 'guayaba_manzana',\n",
       "       'guayaba_pera', 'higo', 'kiwi', 'limon_tahiti', 'limon_comun',\n",
       "       'limon_comun_cienaga', 'limon_comun_valluno', 'limon_mandarino',\n",
       "       'lulo', 'mandarina_oneco', 'mandarina_arrayana', 'mandarina_comun',\n",
       "       'mango_tommy', 'mango_comun', 'mango_de_azucar', 'mango_manzano',\n",
       "       'mango_reina', 'manzana_roja_importada',\n",
       "       'manzana_royal_gala_importada', 'maracuya',\n",
       "       'manzana_verde_importada', 'maracuya_antioqueno',\n",
       "       'maracuya_santandereano', 'melon_cantalup', 'mora_de_castilla',\n",
       "       'naranja_valencia', 'naranja_comun', 'papaya_maradol',\n",
       "       'papaya_hawaiana', 'papaya_melona', 'papaya_redonda', 'patilla',\n",
       "       'pera_importada', 'pitahaya', 'pina_gold', 'pina_manzana',\n",
       "       'pina_perolera', 'tangelo', 'tomate_de_arbol',\n",
       "       'uchuva_con_cascara', 'uva_isabela', 'uva_importada', 'uva_negra',\n",
       "       'uva_red_globe_nacional', 'uva_roja', 'uva_verde', 'zapote',\n",
       "       'arracacha_amarilla', 'arracacha_blanca', 'papa_ica-huila',\n",
       "       'papa_morasurco', 'papa_purace', 'papa_r-12_negra',\n",
       "       'papa_r-12_roja', 'papa_capira', 'papa_criolla_limpia',\n",
       "       'papa_criolla_sucia', 'papa_nevada', 'papa_parda_pastusa',\n",
       "       'papa_roja_peruana', 'papa_ruby', 'papa_sabanera', 'papa_suprema',\n",
       "       'papa_unica', 'platano_comino', 'platano_dominico_harton_maduro',\n",
       "       'platano_dominico_harton_verde', 'platano_dominico_verde',\n",
       "       'platano_guineo', 'platano_harton_maduro', 'platano_harton_verde',\n",
       "       'platano_harton_verde_llanero', 'ulluco', 'yuca_ica',\n",
       "       'yuca_chirosa', 'yuca_criolla', 'yuca_llanera', 'name_criollo',\n",
       "       'name_diamante', 'name_espino', 'arroz_de_primera',\n",
       "       'arroz_de_segunda', 'arroz_excelso', 'arroz_sopa_cristal',\n",
       "       'arveja_amarilla_seca_importada', 'arveja_enlatada',\n",
       "       'arveja_verde_seca_importada', 'cuchuco_de_cebada',\n",
       "       'cuchuco_de_maiz', 'frijol_uribe_rosado', 'frijol_zaragoza',\n",
       "       'frijol_bolon', 'frijol_cabeza_negra_importado',\n",
       "       'frijol_cabeza_negra_nacional', 'frijol_calima',\n",
       "       'frijol_cargamanto_blanco', 'frijol_cargamanto_rojo',\n",
       "       'frijol_enlatado', 'frijol_nima_calima',\n",
       "       'frijol_palomito_importado', 'frijol_radical',\n",
       "       'garbanzo_importado', 'lenteja_importada', 'maiz_amarillo_cascara',\n",
       "       'maiz_amarillo_trillado', 'maiz_blanco_retrillado',\n",
       "       'maiz_blanco_trillado', 'maiz_pira', 'huevo_blanco_a',\n",
       "       'huevo_blanco_aa', 'huevo_blanco_b', 'huevo_blanco_extra',\n",
       "       'huevo_rojo_a', 'huevo_rojo_aa', 'huevo_rojo_b',\n",
       "       'huevo_rojo_extra', 'leche_en_polvo', 'queso_campesino',\n",
       "       'queso_costeno', 'queso_cuajada', 'queso_doble_crema',\n",
       "       'alas_de_pollo_con_costillar', 'alas_de_pollo_sin_costillar',\n",
       "       'carne_de_cerdo_en_canal', 'carne_de_cerdo_brazo_con_hueso',\n",
       "       'carne_de_cerdo_brazo_sin_hueso', 'carne_de_cerdo_cabeza_de_lomo',\n",
       "       'carne_de_cerdo_costilla', 'carne_de_cerdo_espinazo',\n",
       "       'carne_de_cerdo_lomo_con_hueso', 'carne_de_cerdo_lomo_sin_hueso',\n",
       "       'carne_de_cerdo_pernil_con_hueso',\n",
       "       'carne_de_cerdo_pernil_sin_hueso', 'carne_de_cerdo_tocino_barriga',\n",
       "       'carne_de_cerdo_tocino_papada', 'carne_de_res_en_canal',\n",
       "       'carne_de_res_molida_murillo', 'carne_de_res_bola_de_brazo',\n",
       "       'carne_de_res_bola_de_pierna', 'carne_de_res_bota',\n",
       "       'carne_de_res_cadera', 'carne_de_res_centro_de_pierna',\n",
       "       'carne_de_res_chatas', 'carne_de_res_cogote',\n",
       "       'carne_de_res_costilla', 'carne_de_res_falda',\n",
       "       'carne_de_res_lomo_de_brazo', 'carne_de_res_lomo_fino',\n",
       "       'carne_de_res_morrillo', 'carne_de_res_muchacho',\n",
       "       'carne_de_res_murillo', 'carne_de_res_paletero',\n",
       "       'carne_de_res_pecho', 'carne_de_res_punta_de_anca',\n",
       "       'carne_de_res_sobrebarriga', 'menudencias_de_pollo',\n",
       "       'muslos_de_pollo_sin_rabadilla', 'pechuga_de_pollo',\n",
       "       'pierna_pernil_con_rabadilla', 'pierna_pernil_sin_rabadilla',\n",
       "       'piernas_de_pollo', 'pollo_entero_congelado_sin_visceras',\n",
       "       'pollo_entero_fresco_sin_visceras', 'rabadillas_de_pollo',\n",
       "       'almejas_con_concha', 'almejas_sin_concha',\n",
       "       'bagre_rayado_en_postas_congelado',\n",
       "       'bagre_rayado_entero_congelado', 'bagre_rayado_entero_fresco',\n",
       "       'blanquillo_entero_fresco', 'bocachico_criollo_fresco',\n",
       "       'bocachico_importado_congelado', 'cachama_de_cultivo_fresca',\n",
       "       'calamar_anillos', 'calamar_blanco_entero',\n",
       "       'calamar_morado_entero', 'camaron_tigre_precocido_seco',\n",
       "       'camaron_titi_precocido_seco', 'capaz_magdalena_fresco',\n",
       "       'cazuela_de_mariscos_paquete', 'corvina_filete_congelado_nacional',\n",
       "       'langostino_16-20', 'merluza_filete_importado',\n",
       "       'merluza_filete_nacional', 'mojarra_lora_entera_congelada',\n",
       "       'mojarra_lora_entera_fresca', 'nicuro_fresco', 'palmitos_de_mar',\n",
       "       'pargo_rojo_entero_congelado', 'pargo_rojo_platero',\n",
       "       'pescado_cabezas', 'robalo_filete_congelado',\n",
       "       'salmon_filete_congelado', 'sierra_entera_congelada',\n",
       "       'tilapia_roja_entera_congelada', 'margarina',\n",
       "       'tilapia_roja_entera_fresca', 'tilapia_filete_congelado',\n",
       "       'toyo_blanco_filete_congelado', 'trucha_en_corte_mariposa',\n",
       "       'trucha_entera_fresca', 'aceite_girasol', 'aceite_vegetal_mezcla',\n",
       "       'avena_en_hojuelas', 'avena_molida', 'azucar_morena',\n",
       "       'azucar_refinada', 'azucar_sulfitada', 'bocadillo_veleno',\n",
       "       'cafe_instantaneo', 'cafe_molido', 'chocolate_amargo',\n",
       "       'chocolate_dulce', 'chocolate_instantaneo', 'color_bolsita',\n",
       "       'fecula_de_maiz', 'galletas_dulces_redondas_con_crema',\n",
       "       'galletas_saladas_3_tacos', 'gelatina', 'harina_de_trigo',\n",
       "       'harina_precocida_de_maiz', 'jugo_de_frutas',\n",
       "       'jugo_instantaneo_sobre', 'lomitos_de_atun_en_lata', 'manteca',\n",
       "       'mayonesa_doy_pack', 'panela_cuadrada_blanca',\n",
       "       'panela_cuadrada_morena', 'panela_redonda_morena',\n",
       "       'pastas_alimenticias', 'sal_yodada', 'salsa_de_tomate_doy_pack',\n",
       "       'sardinas_en_lata', 'vinagre', 'papa_parda_para_lavar',\n",
       "       'maiz_blanco_cascara', 'pargo_rojo_entero_fresco',\n",
       "       'papa_tocarrena', 'manzana_nacional', 'maracuya_valluno',\n",
       "       'pera_nacional', 'maiz_enlatado', 'muslos_de_pollo_con_rabadilla',\n",
       "       'langostino_u12', 'sopa_de_pollo_caja', 'cebolla_cabezona_roja',\n",
       "       'platano_harton_verde_ecuatoriano', 'repollo_morado_antioqueno',\n",
       "       'maracuya_huilense', 'limon_comun_ecuatoriano', 'mango_costeno',\n",
       "       'pollo_entero_fresco_con_visceras',\n",
       "       'carne_de_cerdo_tocineta_plancha', 'queso_caqueta',\n",
       "       'pollo_entero_congelado_con_visceras', 'aceite_soya', 'papa_rubi',\n",
       "       'papa_superior', 'naranja_sweet', 'papa_betina', 'curuba',\n",
       "       'gulupa', 'ahuyamin_sakata', 'repollo_verde', 'patilla_baby',\n",
       "       'tomate_rinon_valluno', 'tomate_chonto_regional',\n",
       "       'ciruela_negra_importada', 'ciruela_roja_importada',\n",
       "       'galletas_saladas', 'ciruela_importada', 'papaya_tainung',\n",
       "       'mango_yulima', 'maiz_amarillo_cascara_importado',\n",
       "       'panela_en_pastilla', 'guayaba_pera_valluna',\n",
       "       'basa_filete_congelado_importado', 'mango_kent',\n",
       "       'panela_redonda_blanca', 'guayaba_atlantico', 'papa_san_felix',\n",
       "       'platano_harton_verde_eje_cafetero', 'arroz_blanco_importado',\n",
       "       'tilapia_lomitos', 'mostaza_doy_pack', 'c',\n",
       "       'basa_entero_congelado_importado', 'papaya_paulina',\n",
       "       'aceite_de_palma'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.producto.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T18:48:26.553003Z",
     "start_time": "2024-10-02T18:48:26.290384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>producto</th>\n",
       "      <th>ciudad</th>\n",
       "      <th>precio_minimo</th>\n",
       "      <th>precio_maximo</th>\n",
       "      <th>precio_medio</th>\n",
       "      <th>tendencia</th>\n",
       "      <th>categoria</th>\n",
       "      <th>mercado</th>\n",
       "      <th>semana_no</th>\n",
       "      <th>anho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1181462</th>\n",
       "      <td>mostaza_doy_pack</td>\n",
       "      <td>cartagena</td>\n",
       "      <td>16667</td>\n",
       "      <td>17500</td>\n",
       "      <td>16875</td>\n",
       "      <td>=</td>\n",
       "      <td>productos_procesados</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186113</th>\n",
       "      <td>mostaza_doy_pack</td>\n",
       "      <td>cartagena</td>\n",
       "      <td>16667</td>\n",
       "      <td>17500</td>\n",
       "      <td>16875</td>\n",
       "      <td>=</td>\n",
       "      <td>productos_procesados</td>\n",
       "      <td>bazurto</td>\n",
       "      <td>21</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190753</th>\n",
       "      <td>mostaza_doy_pack</td>\n",
       "      <td>cartagena</td>\n",
       "      <td>16667</td>\n",
       "      <td>17500</td>\n",
       "      <td>16875</td>\n",
       "      <td>=</td>\n",
       "      <td>productos_procesados</td>\n",
       "      <td>bazurto</td>\n",
       "      <td>22</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195383</th>\n",
       "      <td>mostaza_doy_pack</td>\n",
       "      <td>cartagena</td>\n",
       "      <td>16667</td>\n",
       "      <td>17500</td>\n",
       "      <td>16875</td>\n",
       "      <td>=</td>\n",
       "      <td>productos_procesados</td>\n",
       "      <td>bazurto</td>\n",
       "      <td>23</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200012</th>\n",
       "      <td>mostaza_doy_pack</td>\n",
       "      <td>cartagena</td>\n",
       "      <td>16667</td>\n",
       "      <td>17500</td>\n",
       "      <td>16875</td>\n",
       "      <td>=</td>\n",
       "      <td>productos_procesados</td>\n",
       "      <td>bazurto</td>\n",
       "      <td>24</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623467</th>\n",
       "      <td>mostaza_doy_pack</td>\n",
       "      <td>barranquilla</td>\n",
       "      <td>13598</td>\n",
       "      <td>13750</td>\n",
       "      <td>13662</td>\n",
       "      <td>+</td>\n",
       "      <td>productos_procesados</td>\n",
       "      <td>granabastos</td>\n",
       "      <td>8</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623468</th>\n",
       "      <td>mostaza_doy_pack</td>\n",
       "      <td>sincelejo</td>\n",
       "      <td>14545</td>\n",
       "      <td>15985</td>\n",
       "      <td>15303</td>\n",
       "      <td>+</td>\n",
       "      <td>productos_procesados</td>\n",
       "      <td>nuevo mercado</td>\n",
       "      <td>8</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627788</th>\n",
       "      <td>mostaza_doy_pack</td>\n",
       "      <td>barranquilla</td>\n",
       "      <td>13580</td>\n",
       "      <td>14098</td>\n",
       "      <td>13891</td>\n",
       "      <td>+</td>\n",
       "      <td>productos_procesados</td>\n",
       "      <td>barranquillita</td>\n",
       "      <td>9</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627789</th>\n",
       "      <td>mostaza_doy_pack</td>\n",
       "      <td>barranquilla</td>\n",
       "      <td>13258</td>\n",
       "      <td>13674</td>\n",
       "      <td>13460</td>\n",
       "      <td>-</td>\n",
       "      <td>productos_procesados</td>\n",
       "      <td>granabastos</td>\n",
       "      <td>9</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627790</th>\n",
       "      <td>mostaza_doy_pack</td>\n",
       "      <td>sincelejo</td>\n",
       "      <td>14545</td>\n",
       "      <td>15985</td>\n",
       "      <td>15303</td>\n",
       "      <td>=</td>\n",
       "      <td>productos_procesados</td>\n",
       "      <td>nuevo mercado</td>\n",
       "      <td>9</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1071 rows Ã 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 producto        ciudad  precio_minimo  precio_maximo  \\\n",
       "1181462  mostaza_doy_pack     cartagena          16667          17500   \n",
       "1186113  mostaza_doy_pack     cartagena          16667          17500   \n",
       "1190753  mostaza_doy_pack     cartagena          16667          17500   \n",
       "1195383  mostaza_doy_pack     cartagena          16667          17500   \n",
       "1200012  mostaza_doy_pack     cartagena          16667          17500   \n",
       "...                   ...           ...            ...            ...   \n",
       "2623467  mostaza_doy_pack  barranquilla          13598          13750   \n",
       "2623468  mostaza_doy_pack     sincelejo          14545          15985   \n",
       "2627788  mostaza_doy_pack  barranquilla          13580          14098   \n",
       "2627789  mostaza_doy_pack  barranquilla          13258          13674   \n",
       "2627790  mostaza_doy_pack     sincelejo          14545          15985   \n",
       "\n",
       "         precio_medio tendencia             categoria         mercado  \\\n",
       "1181462         16875         =  productos_procesados            None   \n",
       "1186113         16875         =  productos_procesados         bazurto   \n",
       "1190753         16875         =  productos_procesados         bazurto   \n",
       "1195383         16875         =  productos_procesados         bazurto   \n",
       "1200012         16875         =  productos_procesados         bazurto   \n",
       "...               ...       ...                   ...             ...   \n",
       "2623467         13662         +  productos_procesados     granabastos   \n",
       "2623468         15303         +  productos_procesados   nuevo mercado   \n",
       "2627788         13891         +  productos_procesados  barranquillita   \n",
       "2627789         13460         -  productos_procesados     granabastos   \n",
       "2627790         15303         =  productos_procesados   nuevo mercado   \n",
       "\n",
       "         semana_no  anho  \n",
       "1181462         20  2018  \n",
       "1186113         21  2018  \n",
       "1190753         22  2018  \n",
       "1195383         23  2018  \n",
       "1200012         24  2018  \n",
       "...            ...   ...  \n",
       "2623467          8  2024  \n",
       "2623468          8  2024  \n",
       "2627788          9  2024  \n",
       "2627789          9  2024  \n",
       "2627790          9  2024  \n",
       "\n",
       "[1071 rows x 10 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['producto']=='mostaza_doy_pack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1018.75px",
    "left": "395px",
    "top": "280.855px",
    "width": "167.5px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
