{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://www.dane.gov.co'\n",
    "url = 'https://www.dane.gov.co/index.php/estadisticas-por-tema/agropecuario/sistema-de-informacion-de-precios-sipsa/mayoristas-boletin-semanal-1'\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\"\n",
    "}\n",
    "response = requests.get(url, headers = headers)\n",
    "if response.status_code == 200:\n",
    "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "  \n",
    "  # Getting link for each year\n",
    "  link_years = soup.find_all(lambda tag: tag.name == 'a' and any(substring in tag.get('title', '').lower() for substring in ['boletín mayorista semanal','boletin mayorista semanal', 'mayoristas boletín semanal']))\n",
    "\n",
    "  # within each year\n",
    "  for link in link_years:\n",
    "      \n",
    "      print(f' Working on {link.text.strip()} data')\n",
    "\n",
    "      # Request for reports available in such year\n",
    "      r = requests.get(url_base + link['href'], headers = headers)\n",
    "      soup_year = BeautifulSoup(r.content, \"html.parser\")\n",
    "      \n",
    "      # Create links for each report within the given year\n",
    "      target_links = [item for item in soup_year.find_all(target = '_blank') if 'Anexo' in item.text]\n",
    "      date_elements = soup_year.find_all(lambda tag: tag.name == 'td' and tag.find('img', src=\"/files/images/boton.png\")) #\n",
    "      \n",
    "\n",
    "      n = len(target_links)\n",
    "\n",
    "      # Create folders per year\n",
    "      REPORTS_PATH = Path.cwd()/'reports'/link.text.strip()\n",
    "      if REPORTS_PATH.exists():\n",
    "          print(f'{REPORTS_PATH} already exists')\n",
    "      else: \n",
    "          REPORTS_PATH.mkdir(parents= True, exist_ok=True)\n",
    "\n",
    "      \n",
    "      # For each link, create file in local and fill it with data \n",
    "      for i, file in tqdm(enumerate(target_links)):\n",
    "          # print(file)\n",
    "          # request information\n",
    "          try: \n",
    "              result = requests.get(url_base+file['href'], headers = headers)\n",
    "          except:\n",
    "              result = requests.get(file['href'], headers = headers)\n",
    "          \n",
    "          # name output file\n",
    "          file_name = 'week_'+str(n-i)+'_'+file['href'].split('/')[-1]\n",
    "          \n",
    "          # export data to local path\n",
    "          with open(REPORTS_PATH/file_name, 'wb') as f: \n",
    "              f.write(result.content)\n",
    "          \n",
    "          # Wait five second before next iteration \n",
    "          time.sleep(1)           \n",
    "\n",
    "else: \n",
    "  print(f'[Fail] Status error:{response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'locale' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Set the locale to Spanish for month names\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m locale\u001b[39m.\u001b[39msetlocale(locale\u001b[39m.\u001b[39mLC_TIME, \u001b[39m'\u001b[39m\u001b[39mes_ES.utf8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Getting link for each year\u001b[39;00m\n\u001b[0;32m      5\u001b[0m link_years \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind_all(\u001b[39mlambda\u001b[39;00m tag: tag\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(substring \u001b[39min\u001b[39;00m tag\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m substring \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mboletín mayorista semanal\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mboletin mayorista semanal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmayoristas boletín semanal\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'locale' is not defined"
     ]
    }
   ],
   "source": [
    "# # Set the locale to Spanish for month names\n",
    "# locale.setlocale(locale.LC_TIME, 'es_ES.utf8')\n",
    "\n",
    "# Getting link for each year\n",
    "link_years = soup.find_all(lambda tag: tag.name == 'a' and any(substring in tag.get('title', '').lower() for substring in ['boletín mayorista semanal','boletin mayorista semanal', 'mayoristas boletín semanal']))\n",
    "\n",
    "# within each year\n",
    "for link in link_years:\n",
    "    \n",
    "    print(f' Working on {link.text.strip()} data')\n",
    "\n",
    "    # Request for reports available in such year\n",
    "    r = requests.get(url_base + link['href'], headers = headers)\n",
    "    soup_year = BeautifulSoup(r.content, \"html.parser\")\n",
    "    \n",
    "    # Create links for each report within the given year\n",
    "    target_links = [item for item in soup_year.find_all(target = '_blank') if 'Anexo' in item.text]\n",
    "    date_elements = soup_year.find_all(lambda tag: tag.name == 'td' and tag.find('img', src=\"/files/images/boton.png\")) #\n",
    "    \n",
    "\n",
    "    n = len(target_links)\n",
    "\n",
    "    # Create folders per year\n",
    "    REPORTS_PATH = Path.cwd()/'reports'/link.text.strip()\n",
    "    if REPORTS_PATH.exists():\n",
    "        print(f'{REPORTS_PATH} already exists')\n",
    "    else: \n",
    "        REPORTS_PATH.mkdir(parents= True, exist_ok=True)\n",
    "\n",
    "    \n",
    "    # For each link, create file in local and fill it with data \n",
    "    for i, file in tqdm(enumerate(target_links)):\n",
    "        # print(file)\n",
    "        # request information\n",
    "        try: \n",
    "            result = requests.get(url_base+file['href'], headers = headers)\n",
    "        except:\n",
    "            result = requests.get(file['href'], headers = headers)\n",
    "        \n",
    "        # name output file\n",
    "        file_name = 'week_'+str(n-i)+'_'+file['href'].split('/')[-1]\n",
    "        \n",
    "        # export data to local path\n",
    "        with open(REPORTS_PATH/file_name, 'wb') as f: \n",
    "            f.write(result.content)\n",
    "        \n",
    "        # Wait five second before next iteration \n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_format_wrangling(path_list):\n",
    "    # Creating target dataframe\n",
    "    year_file = pd.DataFrame()\n",
    "\n",
    "    # Creating food categories \n",
    "    categories_dict = {1: 'verduras_hortalizas',\n",
    "        2: 'frutas_frescas',\n",
    "        3: 'tuberculos_raices_platanos',\n",
    "        4: 'granos_cereales',\n",
    "        5: 'huevos_lacteos',\n",
    "        6: 'carnes',\n",
    "        7: 'pescados',\n",
    "        8: 'productos_procesados'}\n",
    "\n",
    "    # Iterating over list of paths \n",
    "    for file_path in tqdm(path_list):\n",
    "        \n",
    "        # Capturing off cases\n",
    "        try: \n",
    "            # Import data\n",
    "            with open(file_path, 'rb') as f: \n",
    "                dataframe = pd.read_excel(f, header = None)\n",
    "            \n",
    "            # Keeping only first five columns and renaming them\n",
    "            dataframe = dataframe.iloc[:,0:5]\n",
    "            dataframe.columns = ['ciudad','precio_minimo','precio_maximo','precio_medio', 'tendencia']\n",
    "            dataframe['ciudad'] = dataframe['ciudad'].str.lower().str.replace('bogotá, d.c.', 'bogota')\n",
    "            \n",
    "            # rows where ciudad is non null\n",
    "            dataframe = dataframe[~dataframe['ciudad'].isnull()]\n",
    "            \n",
    "            # This formatting would have eight food categories within the same spreadsheet divided only by a big title. \n",
    "            # Such title would include the word 'cuadro'. So, to separate categories, we look for blocks of data contained \n",
    "            # within two consecutive appareances of such words. \n",
    "\n",
    "            # Get row indexes where the word 'cuadro' is present\n",
    "            index_cuadro = dataframe[dataframe['ciudad'].str.contains('cuadro')].index\n",
    "\n",
    "            # Creating target dataframe for all data\n",
    "            df_final= pd.DataFrame()\n",
    "\n",
    "            # Itearting over food categories. \n",
    "            for i_categoria in range(len(index_cuadro)):\n",
    "                \n",
    "                # Capturing first seven categories\n",
    "                if i_categoria < 7:\n",
    "                    dataframe_categoria = dataframe[index_cuadro[i_categoria]+2:index_cuadro[i_categoria+1]]\n",
    "                # Capturing last category\n",
    "                else: \n",
    "                    dataframe_categoria = dataframe[index_cuadro[i_categoria]+2:]\n",
    "\n",
    "                # Within each category block, add category name\n",
    "                dataframe_categoria['categoria'] = categories_dict[i_categoria+1]\n",
    "\n",
    "                # within each category block, there are several products. In the whole reporting, products are very likely to contain \n",
    "                # several rows (same food item in different locations). What identifies such product blocks is the fact that the precio_minimo\n",
    "                # column will be blank. So the product data would be contain within two consecutive occurrencies of blank prices. \n",
    "                index_producto = dataframe_categoria[dataframe_categoria['precio_minimo'].isnull()].index\n",
    "\n",
    "                # creating target data frame for product category\n",
    "                df_categoria_final = pd.DataFrame()\n",
    "\n",
    "                # Iterating over products within food category\n",
    "                for i_producto in range(len(index_producto)): \n",
    "                    \n",
    "                    # Capturing the first product in the category \n",
    "                    if i_producto == 0:\n",
    "                        dataframe_producto = dataframe_categoria.loc[index_producto[i_producto]-1:index_producto[i_producto+1]-1].reset_index(drop = True)\n",
    "                    \n",
    "                    # Capturing all intermediate products\n",
    "                    elif i_producto < len(index_producto)-1: \n",
    "                        dataframe_producto = dataframe_categoria.loc[index_producto[i_producto]:index_producto[i_producto+1]-1].reset_index(drop = True)\n",
    "                    \n",
    "                    # Capturing last product within category \n",
    "                    else: \n",
    "                        dataframe_producto = dataframe_categoria.loc[index_producto[i_producto]:].reset_index(drop = True)\n",
    "                    \n",
    "                    # Adding product name column to each block of products\n",
    "                    dataframe_producto['producto'] = dataframe_producto['ciudad'][0]\n",
    "\n",
    "                    # Keeping only city name under the ciudad column\n",
    "                    dataframe_producto['ciudad'] = dataframe_producto['ciudad'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n",
    "                    \n",
    "                    # The name of the marketplaces is included on some of the city names. So we try to retrieve it\n",
    "                    try: \n",
    "                        dataframe_producto['mercado'] = dataframe_producto['ciudad'].str.split(',').str[1].str.strip()\n",
    "                    except:\n",
    "                        dataframe_producto['mercado'] = np.nan\n",
    "                    \n",
    "                    # Getting a clean version of city name\n",
    "                    try: \n",
    "                        dataframe_producto['ciudad'] = dataframe_producto['ciudad'].str.split(',').str[0].str.strip()\n",
    "                    except: \n",
    "                        None\n",
    "                    # Dropping first row\n",
    "                    dataframe_producto = dataframe_producto.drop(0)\n",
    "\n",
    "                    # Putting together all data for products within food category \n",
    "                    df_categoria_final = pd.concat([df_categoria_final, dataframe_producto], ignore_index = True)\n",
    "\n",
    "                # Putting together all data \n",
    "                df_final = pd.concat([df_final,df_categoria_final], ignore_index = True)\n",
    "\n",
    "            # Once data per file is complete, time stamps are added: year and week number\n",
    "            df_final['semana_no'] = int(file_path.name.split('_')[1])#file_path.stem[5:7]\n",
    "            df_final['anho'] = file_path.stem[-4:]\n",
    "            \n",
    "            # Adding data file to main target dataframe\n",
    "            year_file = pd.concat([year_file,df_final], ignore_index = True)\n",
    "            year_file = year_file[['anho','semana_no','categoria','producto','ciudad','precio_minimo','precio_maximo','precio_medio','tendencia']]\n",
    "        \n",
    "        # Printing what went wrong\n",
    "        except Exception as e: \n",
    "            print(f'[Info] There is a problem in {file_path}: \\n {e}')\n",
    "    \n",
    "    return year_file;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_format_paths():\n",
    "    # Overall files are consistent with formating, but there are two major types of formats. First of them would apply up to week 19 of 2018. \n",
    "    first_format = ['2018', '2019', '2020', '2021', '2022', '2023']\n",
    "    final_files_paths = []\n",
    "\n",
    "    # Putting together overall list of files\n",
    "    for year in first_format: \n",
    "        \n",
    "        # Collecting paths from each year\n",
    "        root_path = Path.cwd()/'reports'/year\n",
    "        \n",
    "        # collecting both xls and xlsx files\n",
    "        xls_files = list(root_path.glob('*.xls'))\n",
    "        xlsx_files = list(root_path.glob('*.xlsx'))\n",
    "        all_files = xls_files + xlsx_files\n",
    "        \n",
    "        # This formatting would only apply after the 20th week of 2018\n",
    "        if year == '2018':\n",
    "            all_files = [path for path in all_files if int(path.stem.split('_')[1]) > 19]\n",
    "        \n",
    "        # Putting together the list of paths\n",
    "        final_files_paths.extend(all_files)\n",
    "    \n",
    "    return final_files_paths;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_format_wrangling(path_list):\n",
    "    \n",
    "    # Creating target dataframe\n",
    "    year_file2 = pd.DataFrame()\n",
    "\n",
    "    # Creating food categories \n",
    "    categories_dict = {1: 'verduras_hortalizas',\n",
    "        2: 'frutas_frescas',\n",
    "        3: 'tuberculos_raices_platanos',\n",
    "        4: 'granos_cereales',\n",
    "        5: 'huevos_lacteos',\n",
    "        6: 'carnes',\n",
    "        7: 'pescados',\n",
    "        8: 'productos_procesados'}\n",
    "\n",
    "    # Iteration over list of paths\n",
    "    for file_path in tqdm(path_list): \n",
    "\n",
    "        # Capturing off cases\n",
    "        try: \n",
    "            # Importing file and extracting book names\n",
    "            xl = pd.ExcelFile(file_path)\n",
    "            ref_dict = {i: xl.sheet_names[i] for i in range(len(xl.sheet_names))}\n",
    "            \n",
    "            # Creating target dataframe for file data\n",
    "            dataframe_file = pd.DataFrame()\n",
    "\n",
    "            # Iterating over tabs within file\n",
    "            for index in range(1,9):\n",
    "                \n",
    "                # Importing file\n",
    "                with open(file_path, 'rb') as f: \n",
    "                    dataframe = pd.read_excel(f, sheet_name = ref_dict[index])\n",
    "\n",
    "                # within this second type of formatting, there is two groups based on a subtle\n",
    "                # detail: In one, data would start at row 10, in the other, data would start at row 11\n",
    "                if pd.isnull(dataframe.iloc[9,0]):\n",
    "                    dataframe = dataframe.iloc[10:,:6]\n",
    "                else: \n",
    "                    dataframe = dataframe.iloc[9:,:6]\n",
    "\n",
    "                # Setting column names\n",
    "                dataframe.columns = ['producto', 'ciudad','precio_minimo','precio_maximo','precio_medio', 'tendencia']\n",
    "                # keeping rows non null for ciudad column only\n",
    "                dataframe = dataframe[~dataframe['ciudad'].isnull()]\n",
    "\n",
    "                # Adding categoria and ciudad info \n",
    "                dataframe['categoria'] = categories_dict[index]\n",
    "                dataframe['ciudad'] = dataframe['ciudad'].str.lower().str.replace('bogotá, d.c.', 'bogota')\n",
    "                dataframe['ciudad'] = dataframe['ciudad'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n",
    "                                \n",
    "                # The name of the marketplaces is included on some of the city names. So we try to retrieve it\n",
    "                try: \n",
    "                    dataframe['mercado'] = dataframe['ciudad'].str.split(',').str[1].str.strip()\n",
    "                except:\n",
    "                    dataframe['mercado'] = np.nan\n",
    "                                \n",
    "                # Getting a clean version of city name\n",
    "                try: \n",
    "                    dataframe['ciudad'] = dataframe['ciudad'].str.split(',').str[0].str.strip()\n",
    "                except: \n",
    "                    pass\n",
    "                \n",
    "                # Once data per file is complete, time stamps are added: year and week number\n",
    "                dataframe['semana_no'] = int(file_path.name.split('_')[1])#file_path.stem[5:7]\n",
    "                dataframe['anho'] = file_path.stem[-4:]\n",
    "\n",
    "                 # Adding data file to main target dataframe\n",
    "                dataframe = dataframe[['anho','semana_no','categoria','producto','ciudad','precio_minimo','precio_maximo','precio_medio','tendencia']]\n",
    "                dataframe_file = pd.concat([dataframe_file, dataframe], ignore_index = True)\n",
    "\n",
    "            # Adding file data to target dataframe \n",
    "            year_file2 = pd.concat([year_file2, dataframe_file], ignore_index = True)\n",
    "        \n",
    "        # Printing what went wrong\n",
    "        except Exception as e: \n",
    "            print(f'[Info] There is a problem in {file_path}: \\n {e}')\n",
    "\n",
    "    return year_file2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformation():\n",
    "\n",
    "    # Starting time counter\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # First format data (All data within same tab)\n",
    "    print('Working on first format batch')\n",
    "    first_batch = first_format_wrangling(first_format_paths())\n",
    "\n",
    "    # Second format data (with tabs within spreadsheets)\n",
    "    print('Working on second format batch')\n",
    "    second_batch = second_format_wrangling(second_format_paths())\n",
    "\n",
    "    # Merging resulting data\n",
    "    final_dataframe = pd.concat([first_batch, second_batch], ignore_index = True)\n",
    "    \n",
    "    # Ending time counter\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    # Computing total processing time \n",
    "    total_time = end_time - start_time\n",
    "    print(f'[Info] Total execution time {total_time:.2f}')\n",
    "\n",
    "    return final_dataframe;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on first format batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/285 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 285/285 [19:56<00:00,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on second format batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 276/276 [14:39<00:00,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Total execution time 2077.61\n"
     ]
    }
   ],
   "source": [
    "full_sipsa = data_transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sipsa = pd.read_csv('reports/full_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anho</th>\n",
       "      <th>semana_no</th>\n",
       "      <th>categoria</th>\n",
       "      <th>producto</th>\n",
       "      <th>ciudad</th>\n",
       "      <th>precio_minimo</th>\n",
       "      <th>precio_maximo</th>\n",
       "      <th>precio_medio</th>\n",
       "      <th>tendencia</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>verduras_hortalizas</td>\n",
       "      <td>acelga</td>\n",
       "      <td>barranquilla</td>\n",
       "      <td>2867</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>2922</td>\n",
       "      <td>+</td>\n",
       "      <td>caribe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>verduras_hortalizas</td>\n",
       "      <td>acelga</td>\n",
       "      <td>bogota</td>\n",
       "      <td>300</td>\n",
       "      <td>333.0</td>\n",
       "      <td>303</td>\n",
       "      <td>--</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>verduras_hortalizas</td>\n",
       "      <td>acelga</td>\n",
       "      <td>bucaramanga</td>\n",
       "      <td>1000</td>\n",
       "      <td>1867.0</td>\n",
       "      <td>1296</td>\n",
       "      <td>--</td>\n",
       "      <td>andina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>verduras_hortalizas</td>\n",
       "      <td>acelga</td>\n",
       "      <td>cali</td>\n",
       "      <td>1050</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1281</td>\n",
       "      <td>+++</td>\n",
       "      <td>pacífico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>verduras_hortalizas</td>\n",
       "      <td>acelga</td>\n",
       "      <td>cali</td>\n",
       "      <td>1050</td>\n",
       "      <td>1433.0</td>\n",
       "      <td>1242</td>\n",
       "      <td>+++</td>\n",
       "      <td>pacífico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537469</th>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>productos_procesados</td>\n",
       "      <td>Vinagre</td>\n",
       "      <td>pamplona</td>\n",
       "      <td>1750</td>\n",
       "      <td>1833.0</td>\n",
       "      <td>1778</td>\n",
       "      <td>=</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537470</th>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>productos_procesados</td>\n",
       "      <td>Vinagre</td>\n",
       "      <td>pereira</td>\n",
       "      <td>6216</td>\n",
       "      <td>6840.0</td>\n",
       "      <td>6588</td>\n",
       "      <td>-</td>\n",
       "      <td>andina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537471</th>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>productos_procesados</td>\n",
       "      <td>Vinagre</td>\n",
       "      <td>santa marta</td>\n",
       "      <td>1525</td>\n",
       "      <td>1583.0</td>\n",
       "      <td>1553</td>\n",
       "      <td>+</td>\n",
       "      <td>caribe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537472</th>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>productos_procesados</td>\n",
       "      <td>Vinagre</td>\n",
       "      <td>sincelejo</td>\n",
       "      <td>1333</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1542</td>\n",
       "      <td>-</td>\n",
       "      <td>caribe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537473</th>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>productos_procesados</td>\n",
       "      <td>Vinagre</td>\n",
       "      <td>valledupar</td>\n",
       "      <td>1333</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1422</td>\n",
       "      <td>+</td>\n",
       "      <td>caribe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2537474 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         anho  semana_no             categoria producto        ciudad  \\\n",
       "0        2012          1   verduras_hortalizas   acelga  barranquilla   \n",
       "1        2012          1   verduras_hortalizas   acelga        bogota   \n",
       "2        2012          1   verduras_hortalizas   acelga   bucaramanga   \n",
       "3        2012          1   verduras_hortalizas   acelga          cali   \n",
       "4        2012          1   verduras_hortalizas   acelga          cali   \n",
       "...       ...        ...                   ...      ...           ...   \n",
       "2537469  2023          9  productos_procesados  Vinagre      pamplona   \n",
       "2537470  2023          9  productos_procesados  Vinagre       pereira   \n",
       "2537471  2023          9  productos_procesados  Vinagre   santa marta   \n",
       "2537472  2023          9  productos_procesados  Vinagre     sincelejo   \n",
       "2537473  2023          9  productos_procesados  Vinagre    valledupar   \n",
       "\n",
       "        precio_minimo  precio_maximo precio_medio tendencia    region  \n",
       "0                2867         3000.0         2922         +    caribe  \n",
       "1                 300          333.0          303        --       NaN  \n",
       "2                1000         1867.0         1296        --    andina  \n",
       "3                1050         1500.0         1281       +++  pacífico  \n",
       "4                1050         1433.0         1242       +++  pacífico  \n",
       "...               ...            ...          ...       ...       ...  \n",
       "2537469          1750         1833.0         1778         =       NaN  \n",
       "2537470          6216         6840.0         6588         -    andina  \n",
       "2537471          1525         1583.0         1553         +    caribe  \n",
       "2537472          1333         1875.0         1542         -    caribe  \n",
       "2537473          1333         1600.0         1422         +    caribe  \n",
       "\n",
       "[2537474 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_sipsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sipsa.to_csv('reports/full_report.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "city_to_region = {\n",
    "    'Barranquilla': 'Caribe',\n",
    "    'Cartagena': 'Caribe',\n",
    "    'Santa Marta': 'Caribe',\n",
    "    'Valledupar': 'Caribe',\n",
    "    'Montería': 'Caribe',\n",
    "    'Sincelejo': 'Caribe',\n",
    "    'Riohacha': 'Caribe',\n",
    "    'Ciénaga': 'Caribe',\n",
    "    'Magangué': 'Caribe',\n",
    "    'Maicao': 'Caribe',\n",
    "    'Turbo': 'Caribe',\n",
    "    'Lorica': 'Caribe',\n",
    "    'Sahagún': 'Caribe',\n",
    "    'Aracataca': 'Caribe',\n",
    "    'El Banco': 'Caribe',\n",
    "    'Girardot': 'Andina',\n",
    "    'Ibagué': 'Andina',\n",
    "    'Neiva': 'Andina',\n",
    "    'Pereira': 'Andina',\n",
    "    'Manizales': 'Andina',\n",
    "    'Armenia': 'Andina',\n",
    "    'Cali': 'Pacífico',\n",
    "    'Buenaventura': 'Pacífico',\n",
    "    'Tuluá': 'Pacífico',\n",
    "    'Palmira': 'Pacífico',\n",
    "    'Pasto': 'Pacífico',\n",
    "    'Popayán': 'Pacífico',\n",
    "    'Tumaco': 'Pacífico',\n",
    "    'Yumbo': 'Pacífico',\n",
    "    'Quibdó': 'Pacífico',\n",
    "    'Bogotá': 'Andina',\n",
    "    'Medellín': 'Andina',\n",
    "    'Bucaramanga': 'Andina',\n",
    "    'Cúcuta': 'Andina',\n",
    "    'Villavicencio': 'Orinoquía',\n",
    "    'Yopal': 'Orinoquía',\n",
    "    'Arauca': 'Orinoquía',\n",
    "    'Florencia': 'Amazonía',\n",
    "    'Mocoa': 'Amazonía',\n",
    "    'Leticia': 'Amazonía',\n",
    "    'Puerto Carreño': 'Orinoquía',\n",
    "    'Mitú': 'Amazonía',\n",
    "    'Inírida': 'Amazonía',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bogota', 'duitama', 'ipiales', 'pamplona', 'sogamoso', 'tunja',\n",
       "       'cartago', 'rionegro', 'san gil', 'socorro', 'chiquinquirá',\n",
       "       'el santuario', 'marinilla', 'cajamarca', 'carmen de viboral',\n",
       "       'la ceja', 'san vicente', 'sonsón', 'peñol', 'santa bárbara',\n",
       "       'yarumal', 'la virginia', 'la unión', 'la parada', 'la dorada',\n",
       "       'charalá', 'güepsa', 'moniquirá', 'puente nacional', 'santana',\n",
       "       'vélez', 'caparrapí', 'nocaima', 'villeta', 'honda', 'ubaté',\n",
       "       'cartagena frigorífico candelaria', 'san marcos', 'alvarado',\n",
       "       'espinal', 'lérida', 'purificación', 'venadillo', 'cereté',\n",
       "       'san gilpanela', 'yolombó', 'malambo', 'el carmen de viboral',\n",
       "       'túquerres', 'san andrés de tumaco', 'ancuyá', 'consacá',\n",
       "       'sandoná', 'ancuya', 'tibasosa', 'san sebastián de mariquita'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "full_sipsa.loc[full_sipsa['region'].isnull(), 'ciudad'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            caribe\n",
       "1               NaN\n",
       "2            andina\n",
       "3          pacífico\n",
       "4          pacífico\n",
       "             ...   \n",
       "2523963         NaN\n",
       "2523964      andina\n",
       "2523965      caribe\n",
       "2523966      caribe\n",
       "2523967      caribe\n",
       "Name: ciudad, Length: 2523968, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['region'] = df['ciudad'].map(city_to_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_to_region = {\n",
    "    'barranquilla': 'caribe',\n",
    "    'cartagena': 'caribe',\n",
    "    'santa marta': 'caribe',\n",
    "    'valledupar': 'caribe',\n",
    "    'montería': 'caribe',\n",
    "    'sincelejo': 'caribe',\n",
    "    'riohacha': 'caribe',\n",
    "    'ciénaga': 'caribe',\n",
    "    'magangué': 'caribe',\n",
    "    'maicao': 'caribe',\n",
    "    'turbo': 'caribe',\n",
    "    'lorica': 'caribe',\n",
    "    'sahagún': 'caribe',\n",
    "    'aracataca': 'caribe',\n",
    "    'el banco': 'caribe',\n",
    "    'girardot': 'andina',\n",
    "    'ibagué': 'andina',\n",
    "    'neiva': 'andina',\n",
    "    'pereira': 'andina',\n",
    "    'manizales': 'andina',\n",
    "    'armenia': 'andina',\n",
    "    'cali': 'pacífico',\n",
    "    'buenaventura': 'pacífico',\n",
    "    'tuluá': 'pacífico',\n",
    "    'palmira': 'pacífico',\n",
    "    'pasto': 'pacífico',\n",
    "    'popayán': 'pacífico',\n",
    "    'tumaco': 'pacífico',\n",
    "    'yumbo': 'pacífico',\n",
    "    'quibdó': 'pacífico',\n",
    "    'bogotá': 'andina',\n",
    "    'medellín': 'andina',\n",
    "    'bucaramanga': 'andina',\n",
    "    'cúcuta': 'andina',\n",
    "    'villavicencio': 'orinoquía',\n",
    "    'yopal': 'orinoquía',\n",
    "    'arauca': 'orinoquía',\n",
    "    'florencia': 'amazonía',\n",
    "    'mocoa': 'amazonía',\n",
    "    'leticia': 'amazonía',\n",
    "    'puerto carreño': 'orinoquía',\n",
    "    'mitú': 'amazonía',\n",
    "    'inírida': 'amazonía',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
